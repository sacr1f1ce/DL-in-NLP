{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel,  BertConfig, BertTokenizerFast\n",
    "\n",
    "# Then what you need from tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAME = \"quarantine\"  # \"vaccines\" \"masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertModel.\n",
      "\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Name of the BERT model to use\n",
    "model_name = 'DeepPavlov/rubert-base-cased-sentence'\n",
    "\n",
    "# Load transformers config and set output_hidden_states to False\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config=config)\n",
    "\n",
    "# Load the Transformers BERT model\n",
    "transformer_model = TFBertModel.from_pretrained(model_name, config=config, from_pt=True)\n",
    "\n",
    "# Load the MainLayer\n",
    "bert = transformer_model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from csv\n",
    "data = pd.read_csv('train.tsv', sep='\\t')\n",
    "\n",
    "# Select required columns\n",
    "data = data[['text', f'{CLASS_NAME}_stance', f'{CLASS_NAME}_argument']]\n",
    "\n",
    "# Set your model output as categorical and save in new label col\n",
    "data['stance_label'] = pd.Categorical(data[f'{CLASS_NAME}_stance'])\n",
    "data['argument_label'] = pd.Categorical(data[f'{CLASS_NAME}_argument'])\n",
    "\n",
    "# Transform your output to numeric\n",
    "data[f'{CLASS_NAME}_stance'] = data['stance_label'].cat.codes\n",
    "data[f'{CLASS_NAME}_argument'] = data['argument_label'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from csv\n",
    "data_test = pd.read_csv('./val_all.tsv', sep='\\t')\n",
    "\n",
    "# Select required columns\n",
    "data_test = data_test[['text', f'{CLASS_NAME}_stance', f'{CLASS_NAME}_argument']]\n",
    "\n",
    "# Set your model output as categorical and save in new label col\n",
    "data_test['stance_label'] = pd.Categorical(data_test[f'{CLASS_NAME}_stance'])\n",
    "data_test['argument_label'] = pd.Categorical(data_test[f'{CLASS_NAME}_argument'])\n",
    "\n",
    "# Transform your output to numeric\n",
    "data_test[f'{CLASS_NAME}_stance'] = data_test['stance_label'].cat.codes\n",
    "data_test[f'{CLASS_NAME}_argument'] = data_test['argument_label'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready output data for the model\n",
    "test_y_stance = to_categorical(data_test[f'{CLASS_NAME}_stance'])\n",
    "test_y_argument = to_categorical(data_test[f'{CLASS_NAME}_argument'])\n",
    "\n",
    "# Tokenize the input (takes some time)\n",
    "test_x = tokenizer(\n",
    "    text=data_test['text'].to_list(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT_MultiLabel_MultiClass\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  177853440   ['input_ids[0][0]']              \n",
      "                                thPooling(last_hidd                                               \n",
      "                                en_state=(None, 256                                               \n",
      "                                , 768),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " pooled_output (Dropout)        (None, 768)          0           ['bert[2][1]']                   \n",
      "                                                                                                  \n",
      " argument (Dense)               (None, 4)            3076        ['pooled_output[0][0]']          \n",
      "                                                                                                  \n",
      " stance (Dense)                 (None, 4)            3076        ['pooled_output[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 177,859,592\n",
      "Trainable params: 177,859,592\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build your model input\n",
    "input_ids = Input(shape=(256,), name='input_ids', dtype='int32')\n",
    "inputs = {'input_ids': input_ids}\n",
    "\n",
    "# Load the Transformers BERT model as a layer in a Keras model\n",
    "bert_model = bert(inputs)[1]\n",
    "dropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\n",
    "pooled_output = dropout(bert_model, training=False)\n",
    "\n",
    "# Then build your model output\n",
    "stance = Dense(units=len(data.stance_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='stance')(pooled_output)\n",
    "argument = Dense(units=len(data.argument_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='argument')(pooled_output)\n",
    "outputs = {'stance': stance, 'argument': argument}\n",
    "\n",
    "# And combine it all in a model object\n",
    "model = Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n",
    "\n",
    "# Take a look at the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fdcaa121680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fdcaa121680> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "840/840 [==============================] - ETA: 0s - loss: 1.1209 - argument_loss: 0.5144 - stance_loss: 0.6065 - argument_accuracy: 0.8219 - stance_accuracy: 0.7919WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fdcc08610e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fdcc08610e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "840/840 [==============================] - 202s 223ms/step - loss: 1.1209 - argument_loss: 0.5144 - stance_loss: 0.6065 - argument_accuracy: 0.8219 - stance_accuracy: 0.7919 - val_loss: 0.7821 - val_argument_loss: 0.2737 - val_stance_loss: 0.5084 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      "840/840 [==============================] - 191s 227ms/step - loss: 0.6004 - argument_loss: 0.2549 - stance_loss: 0.3455 - argument_accuracy: 0.9333 - stance_accuracy: 0.8770 - val_loss: 0.5781 - val_argument_loss: 0.1532 - val_stance_loss: 0.4249 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "840/840 [==============================] - 189s 225ms/step - loss: 0.5645 - argument_loss: 0.2369 - stance_loss: 0.3276 - argument_accuracy: 0.9367 - stance_accuracy: 0.8803 - val_loss: 0.5537 - val_argument_loss: 0.1497 - val_stance_loss: 0.4040 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "840/840 [==============================] - 190s 227ms/step - loss: 0.5524 - argument_loss: 0.2309 - stance_loss: 0.3215 - argument_accuracy: 0.9375 - stance_accuracy: 0.8809 - val_loss: 0.5944 - val_argument_loss: 0.1557 - val_stance_loss: 0.4387 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "840/840 [==============================] - 190s 226ms/step - loss: 0.5461 - argument_loss: 0.2273 - stance_loss: 0.3188 - argument_accuracy: 0.9379 - stance_accuracy: 0.8809 - val_loss: 0.5425 - val_argument_loss: 0.1463 - val_stance_loss: 0.3962 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "840/840 [==============================] - 189s 226ms/step - loss: 0.5385 - argument_loss: 0.2237 - stance_loss: 0.3148 - argument_accuracy: 0.9388 - stance_accuracy: 0.8813 - val_loss: 0.5971 - val_argument_loss: 0.1739 - val_stance_loss: 0.4232 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "840/840 [==============================] - 189s 226ms/step - loss: 0.5321 - argument_loss: 0.2202 - stance_loss: 0.3119 - argument_accuracy: 0.9397 - stance_accuracy: 0.8819 - val_loss: 0.6986 - val_argument_loss: 0.2100 - val_stance_loss: 0.4887 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "840/840 [==============================] - 190s 226ms/step - loss: 0.5330 - argument_loss: 0.2212 - stance_loss: 0.3118 - argument_accuracy: 0.9393 - stance_accuracy: 0.8818 - val_loss: 0.5269 - val_argument_loss: 0.1486 - val_stance_loss: 0.3782 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "840/840 [==============================] - 189s 225ms/step - loss: 0.5280 - argument_loss: 0.2180 - stance_loss: 0.3100 - argument_accuracy: 0.9388 - stance_accuracy: 0.8810 - val_loss: 0.6866 - val_argument_loss: 0.2217 - val_stance_loss: 0.4649 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "840/840 [==============================] - 189s 225ms/step - loss: 0.5242 - argument_loss: 0.2163 - stance_loss: 0.3078 - argument_accuracy: 0.9394 - stance_accuracy: 0.8818 - val_loss: 0.5943 - val_argument_loss: 0.1712 - val_stance_loss: 0.4231 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "840/840 [==============================] - 189s 225ms/step - loss: 0.5099 - argument_loss: 0.2101 - stance_loss: 0.2998 - argument_accuracy: 0.9410 - stance_accuracy: 0.8828 - val_loss: 0.5455 - val_argument_loss: 0.1444 - val_stance_loss: 0.4011 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "840/840 [==============================] - 189s 226ms/step - loss: 0.4995 - argument_loss: 0.2048 - stance_loss: 0.2947 - argument_accuracy: 0.9425 - stance_accuracy: 0.8845 - val_loss: 0.5864 - val_argument_loss: 0.1721 - val_stance_loss: 0.4143 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "840/840 [==============================] - 190s 226ms/step - loss: 0.5032 - argument_loss: 0.2069 - stance_loss: 0.2963 - argument_accuracy: 0.9416 - stance_accuracy: 0.8837 - val_loss: 0.5667 - val_argument_loss: 0.1597 - val_stance_loss: 0.4071 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "840/840 [==============================] - 189s 226ms/step - loss: 0.4935 - argument_loss: 0.2012 - stance_loss: 0.2923 - argument_accuracy: 0.9424 - stance_accuracy: 0.8840 - val_loss: 0.6293 - val_argument_loss: 0.1803 - val_stance_loss: 0.4490 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "840/840 [==============================] - 189s 225ms/step - loss: 0.4940 - argument_loss: 0.2026 - stance_loss: 0.2914 - argument_accuracy: 0.9428 - stance_accuracy: 0.8845 - val_loss: 0.5823 - val_argument_loss: 0.1559 - val_stance_loss: 0.4265 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "840/840 [==============================] - 190s 226ms/step - loss: 0.4873 - argument_loss: 0.1985 - stance_loss: 0.2888 - argument_accuracy: 0.9436 - stance_accuracy: 0.8848 - val_loss: 0.5283 - val_argument_loss: 0.1367 - val_stance_loss: 0.3916 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "840/840 [==============================] - 190s 226ms/step - loss: 0.4828 - argument_loss: 0.1959 - stance_loss: 0.2869 - argument_accuracy: 0.9437 - stance_accuracy: 0.8845 - val_loss: 0.5371 - val_argument_loss: 0.1410 - val_stance_loss: 0.3961 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "840/840 [==============================] - 189s 225ms/step - loss: 0.4897 - argument_loss: 0.2003 - stance_loss: 0.2894 - argument_accuracy: 0.9433 - stance_accuracy: 0.8845 - val_loss: 0.5956 - val_argument_loss: 0.1736 - val_stance_loss: 0.4220 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "840/840 [==============================] - 190s 226ms/step - loss: 0.4848 - argument_loss: 0.1970 - stance_loss: 0.2878 - argument_accuracy: 0.9439 - stance_accuracy: 0.8848 - val_loss: 0.5376 - val_argument_loss: 0.1442 - val_stance_loss: 0.3933 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "840/840 [==============================] - 190s 226ms/step - loss: 0.4881 - argument_loss: 0.1991 - stance_loss: 0.2891 - argument_accuracy: 0.9437 - stance_accuracy: 0.8851 - val_loss: 0.3779 - val_argument_loss: 0.0865 - val_stance_loss: 0.2915 - val_argument_accuracy: 1.0000 - val_stance_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Set an optimizer\n",
    "optimizer = Adam(\n",
    "    learning_rate=5e-05,\n",
    "    epsilon=1e-08,\n",
    "    decay=0.01,\n",
    "    clipnorm=1.0)\n",
    "\n",
    "# Set loss and metrics\n",
    "loss = {'stance': CategoricalCrossentropy(from_logits = True), 'argument': CategoricalCrossentropy(from_logits = True)}\n",
    "metric = {'stance': CategoricalAccuracy('accuracy'), 'argument': CategoricalAccuracy('accuracy')}\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = loss, \n",
    "    metrics = metric)\n",
    "\n",
    "# Ready output data for the model\n",
    "y_stance = to_categorical(data[f'{CLASS_NAME}_stance'])\n",
    "y_argument = to_categorical(data[f'{CLASS_NAME}_argument'])\n",
    "\n",
    "# Tokenize the input (takes some time)\n",
    "x = tokenizer(\n",
    "    text=data['text'].to_list(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    truncation=True,\n",
    "    padding=True, \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    # x={'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']},\n",
    "    x={'input_ids': x['input_ids']},\n",
    "    y={'stance': y_stance, 'argument': y_argument},\n",
    "    validation_data=({'input_ids': test_x['input_ids'][:8]}, {'stance': test_y_stance[:8], 'argument': test_y_argument[:8]}),\n",
    "    batch_size=8,\n",
    "    epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results = model.predict(x={'input_ids': test_x['input_ids']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[f'{CLASS_NAME}_stance_predict'] = val_results['stance'].argmax(axis=-1)\n",
    "data_test[f'{CLASS_NAME}_argument_predict'] = val_results['argument'].argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       977\n",
      "           1       0.00      0.00      0.00        39\n",
      "           2       0.65      0.98      0.78       290\n",
      "           3       0.00      0.00      0.00       125\n",
      "\n",
      "    accuracy                           0.88      1431\n",
      "   macro avg       0.41      0.50      0.44      1431\n",
      "weighted avg       0.80      0.88      0.84      1431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(data_test[f'{CLASS_NAME}_stance'].values.tolist(), val_results['stance'].argmax(axis=-1), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       977\n",
      "           1       0.00      0.00      0.00        39\n",
      "           2       0.81      0.96      0.88       369\n",
      "           3       0.00      0.00      0.00        46\n",
      "\n",
      "    accuracy                           0.93      1431\n",
      "   macro avg       0.45      0.49      0.47      1431\n",
      "weighted avg       0.88      0.93      0.90      1431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(data_test[f'{CLASS_NAME}_argument'].values.tolist(), val_results['argument'].argmax(axis=-1), zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[f'{CLASS_NAME}_stance_predict'] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[f'{CLASS_NAME}_argument_predict'] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[['text', f'{CLASS_NAME}_stance_predict', f'{CLASS_NAME}_argument_predict']].to_csv(f\"val_predict_{CLASS_NAME}.tsv\", sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(f\"val_predict_{CLASS_NAME}.tsv\", sep='\\t')\n",
    "df1.rename(columns={\"quarantine_stance_predict\": \"quarantine_stance\",\n",
    "                    \"quarantine_argument_predict\": \"quarantine_argument\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for concatentation of all files with results for masks, vaccines and quarantine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv(\"val_predict_masks.tsv\", sep='\\t')\n",
    "# df2.rename(columns={\"masks_stance_predict\": \"masks_stance\", \n",
    "#                        \"masks_argument_predict\": \"masks_argument\"}, inplace=True)\n",
    "\n",
    "# df3 = pd.read_csv(\"val_predict_vaccines.tsv\", sep='\\t')\n",
    "# df3.rename(columns={\"vaccines_stance_predict\": \"vaccines_stance\", \n",
    "#                        \"vaccines_argument_predict\": \"vaccines_argument\",}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = pd.merge(df1, df2, on=\"text\")\n",
    "# result = pd.merge(result, df3, on=\"text\")\n",
    "# result.to_csv(\"val_predict_concat.tsv\", sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: val_predict_concat.tsv (deflated 73%)\n"
     ]
    }
   ],
   "source": [
    "# !zip val_predict_concat.zip val_predict_concat.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
