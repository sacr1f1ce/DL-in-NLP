{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment - Transliteration"
      ],
      "metadata": {
        "id": "KslYmMKMLY7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this task you are required to solve the transliteration problem of names from English to Russian. Transliteration of a string means writing this string using the alphabet of another language with the preservation of pronunciation, although not always.\n"
      ],
      "metadata": {
        "id": "NeC1UiR2LXiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "XvGHiezwIyt_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete the assignment please do the following  steps (both are requred to get the full credits): \n",
        "\n",
        "###1. Complete this notebook\n",
        "\n",
        "Upload a filled notebook with code (this file). You will be asked to implement a transformer-based approach for transliteration.\n",
        "\n",
        "You should implement your ``train`` and ``classify`` functions in this notebook in the cells below. Your model should be implemented as a special class/function in this notebook (be sure if you add any outer dependencies that everything is improted correctly and can be reproducable). \n",
        "\n",
        "\n",
        "###2. Submit solution to the shared task\n",
        "\n",
        "After the implementation of models' architectures you are asked to participate in the [competition](https://competitions.codalab.org/competitions/30932) to solve **Transliteration** task using your implemented code. \n",
        "\n",
        "You should use your code from the previous part to train, validate, and generate predictions for the public (Practice) and private (Evaluation) test sets. It will produce predictions (`preds_translit.tsv`) for the dataset and score them if the true answers are present. You can use these scores to evaluate your model on dev set and choose the best one. Be sure to download the [dataset](https://github.com/skoltech-nlp/filimdb_evaluation/blob/master/TRANSLIT.tar.gz) and unzip it with `wget` command and run them from notebook cells. \n",
        "\n",
        "Upload obtained TSV file with your predictions (``preds_translit.tsv``) in ``.zip`` for the best results to both phases of the competition.\n",
        "\n",
        "\n",
        "**Important: You must indicate \"DL4NLP-23\" as your team name in Codalab. Without it your submission will be invalid!**\n"
      ],
      "metadata": {
        "id": "Mop6m_5rIzu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic algorithm"
      ],
      "metadata": {
        "id": "GIr56czmR5FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic algorithm is based on the following idea: for transliteration, alphabetic n-grams from one language can be transformed into another language into n-grams of the same size, using the most frequent transformation rule found according to statistics on the training sample. \n",
        "\n",
        "To test the implementation, download the data, unzip the datasets, predict transliteration and run the evaluation script. To do this, you need to run the following commands:"
      ],
      "metadata": {
        "id": "F9meQsrCR9xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/s-nlp/filimdb_evaluation/raw/master/TRANSLIT.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lf3-DST1YBx",
        "outputId": "f34ef821-0a75-42d6-f2bd-d1e66500013a",
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-13 09:27:59--  https://github.com/s-nlp/filimdb_evaluation/raw/master/TRANSLIT.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/s-nlp/filimdb_evaluation/master/TRANSLIT.tar.gz [following]\n",
            "--2023-04-13 09:27:59--  https://raw.githubusercontent.com/s-nlp/filimdb_evaluation/master/TRANSLIT.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1546458 (1.5M) [application/octet-stream]\n",
            "Saving to: ‘TRANSLIT.tar.gz’\n",
            "\n",
            "TRANSLIT.tar.gz     100%[===================>]   1.47M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-04-13 09:27:59 (79.1 MB/s) - ‘TRANSLIT.tar.gz’ saved [1546458/1546458]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip TRANSLIT.tar.gz"
      ],
      "metadata": {
        "id": "a1wUwZbT1lDd",
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf TRANSLIT.tar"
      ],
      "metadata": {
        "id": "pg5z4ezh1zO6",
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline code"
      ],
      "metadata": {
        "id": "umowx2OK4IJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Any\n",
        "from random import random\n",
        "import collections as col\n",
        "\n",
        "def baseline_train(\n",
        "        train_source_strings: List[str],\n",
        "        train_target_strings: List[str]) -> Any:\n",
        "    \"\"\"\n",
        "    Trains transliretation model on the given train set represented as\n",
        "    parallel list of input strings and their transliteration via labels.\n",
        "    :param train_source_strings: a list of strings, one str per example\n",
        "    :param train_target_strings: a list of strings, one str per example\n",
        "    :return: learnt parameters, or any object you like (it will be passed to the classify function)\n",
        "    \"\"\"\n",
        "\n",
        "    ngram_lvl = 3\n",
        "    def obtain_train_dicts(train_source_strings, train_target_strings,\n",
        "                            ngram_lvl):\n",
        "        ngrams_dict = col.defaultdict(lambda: col.defaultdict(int))\n",
        "        for src_str,dst_str in zip(train_source_strings,\n",
        "                                        train_target_strings):\n",
        "            try:\n",
        "                src_ngrams = [src_str[i:i+ngram_lvl] for i in\n",
        "                                range(len(src_str)-ngram_lvl+1)]\n",
        "                dst_ngrams = [dst_str[i:i+ngram_lvl] for i in\n",
        "                                range(len(dst_str)-ngram_lvl+1)]\n",
        "            except TypeError as e:\n",
        "                print(src_ngrams, dst_ngrams)\n",
        "                print(e)\n",
        "                raise StopIteration\n",
        "            for src_ngram in src_ngrams:\n",
        "                for dst_ngram in dst_ngrams:\n",
        "                    ngrams_dict[src_ngram][dst_ngram] += 1\n",
        "        return ngrams_dict\n",
        "        \n",
        "    ngrams_dict = col.defaultdict(lambda: col.defaultdict(int))\n",
        "    for nl in range(1, ngram_lvl+1):\n",
        "        ngrams_dict.update(\n",
        "            obtain_train_dicts(train_source_strings,\n",
        "                            train_target_strings, nl))\n",
        "    return ngrams_dict \n",
        "\n",
        "\n",
        "def baseline_classify(strings: List[str], params: Any) -> List[str]:\n",
        "    \"\"\"\n",
        "    Classify strings given previously learnt parameters.\n",
        "    :param strings: strings to classify\n",
        "    :param params: parameters received from train function\n",
        "    :return: list of lists of predicted transliterated strings\n",
        "      (for each source string -> [top_1 prediction, .., top_k prediction]\n",
        "        if it is possible to generate more than one, otherwise\n",
        "        -> [prediction])\n",
        "        corresponding to the given list of strings\n",
        "    \"\"\"\n",
        "       \n",
        "    def predict_one_sample(sample, train_dict, ngram_lvl=1):\n",
        "        ngrams = [sample[i:i+ngram_lvl] for i in\n",
        " range(0,(len(sample) // ngram_lvl * ngram_lvl)-ngram_lvl+1, ngram_lvl)] +\\\n",
        "                 ([] if len(sample) % ngram_lvl == 0 else\n",
        "                    [sample[-(len(sample) % ngram_lvl):]])\n",
        "        prediction = ''\n",
        "        for ngram in ngrams:\n",
        "            ngram_dict = train_dict[ngram]\n",
        "            if len(ngram_dict.keys()) == 0:\n",
        "                prediction += '?'*len(ngram)\n",
        "            else:\n",
        "                prediction += max(ngram_dict, key=lambda k: ngram_dict[k])\n",
        "        return prediction \n",
        "    \n",
        "    ngram_lvl = 3\n",
        "    predictions = []\n",
        "    ngrams_dict = params\n",
        "    for string in strings:\n",
        "        top_1_pred = predict_one_sample(string, ngrams_dict,\n",
        "                                                ngram_lvl)\n",
        "        predictions.append([top_1_pred])\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "U0B2Vyk-4GvE",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:26:35.333041Z",
          "iopub.execute_input": "2023-04-12T09:26:35.333472Z",
          "iopub.status.idle": "2023-04-12T09:26:35.348289Z",
          "shell.execute_reply.started": "2023-04-12T09:26:35.333427Z",
          "shell.execute_reply": "2023-04-12T09:26:35.347193Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation code"
      ],
      "metadata": {
        "id": "r7p9Nac-4X8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PREDS_FNAME = \"preds_translit_baseline.tsv\"\n",
        "SCORED_PARTS = ('train', 'dev', 'train_small', 'dev_small', 'test')\n",
        "TRANSLIT_PATH = \"TRANSLIT\""
      ],
      "metadata": {
        "id": "aOz9Miec58Tb",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:26:35.350108Z",
          "iopub.execute_input": "2023-04-12T09:26:35.350868Z",
          "iopub.status.idle": "2023-04-12T09:26:35.363373Z",
          "shell.execute_reply.started": "2023-04-12T09:26:35.350829Z",
          "shell.execute_reply": "2023-04-12T09:26:35.362300Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "from pandas import read_csv\n",
        "\n",
        "def load_dataset(data_dir_path=None, parts: List[str] = SCORED_PARTS):\n",
        "    part2ixy = {}\n",
        "    for part in parts:\n",
        "        path = os.path.join(data_dir_path, f'{part}.tsv')\n",
        "        with open(path, 'r', encoding='utf-8') as rf:\n",
        "            # first line is a header of the corresponding columns\n",
        "            lines = rf.readlines()[1:]\n",
        "            col_count = len(lines[0].strip('\\n').split('\\t'))\n",
        "            if col_count == 2:\n",
        "                strings, transliterations = zip(\n",
        "                    *list(map(lambda l: l.strip('\\n').split('\\t'), lines))\n",
        "                )\n",
        "            elif col_count == 1:\n",
        "                strings = list(map(lambda l: l.strip('\\n'), lines))\n",
        "                transliterations = None\n",
        "            else:\n",
        "                raise ValueError(\"wrong amount of columns\")\n",
        "        part2ixy[part] = (\n",
        "            [f'{part}/{i}' for i in range(len(strings))],\n",
        "            strings, transliterations,\n",
        "        )\n",
        "    return part2ixy\n",
        "\n",
        "\n",
        "def load_transliterations_only(data_dir_path=None, parts: List[str] = SCORED_PARTS):\n",
        "    part2iy = {}\n",
        "    for part in parts:\n",
        "        path = os.path.join(data_dir_path, f'{part}.tsv')\n",
        "        with open(path, 'r', encoding='utf-8') as rf:\n",
        "            # first line is a header of the corresponding columns\n",
        "            lines = rf.readlines()[1:]\n",
        "            col_count = len(lines[0].strip('\\n').split('\\t'))\n",
        "            n_lines = len(lines)\n",
        "            if col_count == 2:\n",
        "                transliterations = [l.strip('\\n').split('\\t')[1] for l in lines]\n",
        "            elif col_count == 1:\n",
        "                transliterations = None\n",
        "            else:\n",
        "                raise ValueError(\"Wrong amount of columns\")\n",
        "        part2iy[part] = (\n",
        "            [f'{part}/{i}' for i in range(n_lines)],\n",
        "            transliterations,\n",
        "        )\n",
        "    return part2iy\n",
        "\n",
        "\n",
        "def save_preds(preds, preds_fname):\n",
        "    \"\"\"\n",
        "    Save classifier predictions in format appropriate for scoring.\n",
        "    \"\"\"\n",
        "    with codecs.open(preds_fname, 'w') as outp:\n",
        "        for idx, preds in preds:\n",
        "            print(idx, *preds, sep='\\t', file=outp)\n",
        "    print('Predictions saved to %s' % preds_fname)\n",
        "\n",
        "\n",
        "def load_preds(preds_fname, top_k=1):\n",
        "    \"\"\"\n",
        "    Load classifier predictions in format appropriate for scoring.\n",
        "    \"\"\"\n",
        "    kwargs = {\n",
        "        \"filepath_or_buffer\": preds_fname,\n",
        "        \"names\": [\"id\", \"pred\"],\n",
        "        \"sep\": '\\t',\n",
        "    }\n",
        "\n",
        "    pred_ids = list(read_csv(**kwargs, usecols=[\"id\"])[\"id\"])\n",
        "\n",
        "    pred_y = {\n",
        "        pred_id: [y]\n",
        "        for pred_id, y in zip(\n",
        "            pred_ids, read_csv(**kwargs, usecols=[\"pred\"])[\"pred\"]\n",
        "        )\n",
        "    }\n",
        "\n",
        "    for y in pred_y.values():\n",
        "        assert len(y) == top_k\n",
        "\n",
        "    return pred_ids, pred_y\n",
        "\n",
        "\n",
        "def compute_hit_k(preds, k=10):\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def compute_mrr(preds):\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def compute_acc_1(preds, true):\n",
        "    right_answers = 0\n",
        "    bonus = 0\n",
        "    for pred, y in zip(preds, true):\n",
        "        if pred[0] == y:\n",
        "            right_answers += 1\n",
        "        elif pred[0] != pred[0] and y == 'нань':\n",
        "            print('Your test file contained empty string, skipping %f and %s' % (pred[0], y))\n",
        "            bonus += 1 # bugfix: skip empty line in test\n",
        "    return right_answers / (len(preds) - bonus)\n",
        "\n",
        "\n",
        "def score(preds, true):\n",
        "    assert len(preds) == len(true), 'inconsistent amount of predictions and ground truth answers'\n",
        "    acc_1 = compute_acc_1(preds, true)\n",
        "    return {'acc@1': acc_1}\n",
        "\n",
        "\n",
        "def score_preds(preds_path, data_dir, parts=SCORED_PARTS):\n",
        "    part2iy = load_transliterations_only(data_dir, parts=parts)\n",
        "    pred_ids, pred_dict = load_preds(preds_path)\n",
        "    # pred_dict = {i:y for i,y in zip(pred_ids, pred_y)}\n",
        "    scores = {}\n",
        "    for part, (true_ids, true_y) in part2iy.items():\n",
        "        if true_y is None:\n",
        "            print('no labels for %s set' % part)\n",
        "            continue\n",
        "        pred_y = [pred_dict[i] for i in true_ids]\n",
        "        score_values = score(pred_y, true_y)\n",
        "        acc_1 = score_values['acc@1']\n",
        "        print('%s set accuracy@1: %.2f' % (part, acc_1))\n",
        "        scores[part] = score_values \n",
        "    return scores"
      ],
      "metadata": {
        "id": "SdbtMBxd52yX",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:26:35.365194Z",
          "iopub.execute_input": "2023-04-12T09:26:35.366015Z",
          "iopub.status.idle": "2023-04-12T09:26:35.387690Z",
          "shell.execute_reply.started": "2023-04-12T09:26:35.365977Z",
          "shell.execute_reply": "2023-04-12T09:26:35.386682Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and predict results"
      ],
      "metadata": {
        "id": "Juwpml7Z8tbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def train_and_predict(translit_path, scored_parts):\n",
        "    top_k = 1\n",
        "    part2ixy = load_dataset(translit_path, parts=scored_parts)\n",
        "    train_ids, train_strings, train_transliterations = part2ixy['train']\n",
        "    print('\\nTraining classifier on %d examples from train set ...' % len(train_strings))\n",
        "    st = time()\n",
        "    params = baseline_train(train_strings, train_transliterations)\n",
        "    print('Classifier trained in %.2fs' % (time() - st))\n",
        "\n",
        "    allpreds = []\n",
        "    for part, (ids, x, y) in part2ixy.items():\n",
        "        print('\\nClassifying %s set with %d examples ...' % (part, len(x)))\n",
        "        st = time()\n",
        "        preds = baseline_classify(x, params)\n",
        "        print('%s set classified in %.2fs' % (part, time() - st))\n",
        "        count_of_values = list(map(len, preds))\n",
        "        assert np.all(np.array(count_of_values) == top_k)\n",
        "        #score(preds, y)\n",
        "        allpreds.extend(zip(ids, preds))\n",
        "\n",
        "    save_preds(allpreds, preds_fname=PREDS_FNAME)\n",
        "    print('\\nChecking saved predictions ...')\n",
        "    return score_preds(preds_path=PREDS_FNAME, data_dir=translit_path, parts=scored_parts)"
      ],
      "metadata": {
        "id": "VvOmwqtQ4URL",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:26:35.389452Z",
          "iopub.execute_input": "2023-04-12T09:26:35.389894Z",
          "iopub.status.idle": "2023-04-12T09:26:35.401900Z",
          "shell.execute_reply.started": "2023-04-12T09:26:35.389848Z",
          "shell.execute_reply": "2023-04-12T09:26:35.400881Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_predict(TRANSLIT_PATH, SCORED_PARTS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ1GS3l28MRY",
        "outputId": "8ba18a81-d0ee-4e60-c21f-0e7285145a7c",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:28:23.701897Z",
          "iopub.execute_input": "2023-04-12T09:28:23.702659Z",
          "iopub.status.idle": "2023-04-12T09:29:20.847245Z",
          "shell.execute_reply.started": "2023-04-12T09:28:23.702617Z",
          "shell.execute_reply": "2023-04-12T09:29:20.846167Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training classifier on 105371 examples from train set ...\n",
            "Classifier trained in 3.64s\n",
            "\n",
            "Classifying train set with 105371 examples ...\n",
            "train set classified in 22.36s\n",
            "\n",
            "Classifying dev set with 26342 examples ...\n",
            "dev set classified in 5.53s\n",
            "\n",
            "Classifying train_small set with 2000 examples ...\n",
            "train_small set classified in 0.37s\n",
            "\n",
            "Classifying dev_small set with 2000 examples ...\n",
            "dev_small set classified in 0.34s\n",
            "\n",
            "Classifying test set with 32926 examples ...\n",
            "test set classified in 5.70s\n",
            "Predictions saved to preds_translit_baseline.tsv\n",
            "\n",
            "Checking saved predictions ...\n",
            "train set accuracy@1: 0.33\n",
            "dev set accuracy@1: 0.31\n",
            "train_small set accuracy@1: 0.34\n",
            "dev_small set accuracy@1: 0.32\n",
            "no labels for test set\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'acc@1': 0.32907536229133255},\n",
              " 'dev': {'acc@1': 0.3112899552046162},\n",
              " 'train_small': {'acc@1': 0.3365},\n",
              " 'dev_small': {'acc@1': 0.323}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer-based approach"
      ],
      "metadata": {
        "id": "VwXaPC4LiUMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To implement your algorithm, use the template code, which needs to be modified.\n",
        "\n",
        "First, you need to add some details in the code of the Transformer architecture, implement the methods of the class `LrScheduler`, which is responsible for updating the learning rate during training.\n",
        "Next, you need to select the hyperparameters for the model according to the proposed guide."
      ],
      "metadata": {
        "id": "iM-9cKhbidfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nePd6qR5_sC-",
        "outputId": "359994f9-621b-4320-a27b-6698603bde37",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:20.848922Z",
          "iopub.execute_input": "2023-04-12T09:29:20.849252Z",
          "iopub.status.idle": "2023-04-12T09:29:31.560856Z",
          "shell.execute_reply.started": "2023-04-12T09:29:20.849225Z",
          "shell.execute_reply": "2023-04-12T09:29:31.559610Z"
        },
        "trusted": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.20.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
            "  Downloading rapidfuzz-2.15.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.20.9 rapidfuzz-2.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools as it\n",
        "import collections as col\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import datetime, time\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as torch_data\n",
        "import itertools as it\n",
        "import collections as col\n",
        "import random\n",
        "\n",
        "import Levenshtein as le"
      ],
      "metadata": {
        "id": "LQ5sfyjhBawp",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:31.563506Z",
          "iopub.execute_input": "2023-04-12T09:29:31.563949Z",
          "iopub.status.idle": "2023-04-12T09:29:33.932803Z",
          "shell.execute_reply.started": "2023-04-12T09:29:31.563881Z",
          "shell.execute_reply": "2023-04-12T09:29:33.931748Z"
        },
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset and embeddings"
      ],
      "metadata": {
        "id": "r6enjcHrD_0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets(data_dir_path, parts):\n",
        "    datasets = {}\n",
        "    for part in parts:\n",
        "        path = os.path.join(data_dir_path, f'{part}.tsv')\n",
        "        datasets[part] = pd.read_csv(path, sep='\\t', na_filter=False)\n",
        "        print(f'Loaded {part} dataset, length: {len(datasets[part])}')\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "RmWG0dDUCFto",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:33.935557Z",
          "iopub.execute_input": "2023-04-12T09:29:33.936273Z",
          "iopub.status.idle": "2023-04-12T09:29:33.942570Z",
          "shell.execute_reply.started": "2023-04-12T09:29:33.936230Z",
          "shell.execute_reply": "2023-04-12T09:29:33.941442Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder:\n",
        "    def __init__(self, load_dir_path=None):\n",
        "        self.lang_keys = ['en', 'ru']\n",
        "        self.directions = ['id2token', 'token2id']\n",
        "        self.service_token_names = {\n",
        "            'pad_token': '<pad>',\n",
        "            'start_token': '<start>',\n",
        "            'unk_token': '<unk>',\n",
        "            'end_token': '<end>'\n",
        "        }\n",
        "        service_id2token = dict(enumerate(self.service_token_names.values()))\n",
        "        service_token2id ={v:k for k,v in service_id2token.items()}\n",
        "        self.service_vocabs = dict(zip(self.directions,\n",
        "                                       [service_id2token, service_token2id]))\n",
        "        if load_dir_path is None:\n",
        "            self.vocabs = {}\n",
        "            for lk in self.lang_keys:\n",
        "                self.vocabs[lk] = copy.deepcopy(self.service_vocabs)\n",
        "        else:\n",
        "            self.vocabs = self.load_vocabs(load_dir_path)\n",
        "    def load_vocabs(self, load_dir_path):\n",
        "        vocabs = {}\n",
        "        load_path = os.path.join(load_dir_path, 'vocabs')\n",
        "        for lk in self.lang_keys:\n",
        "            vocabs[lk] = {}\n",
        "            for d in self.directions:\n",
        "                columns = d.split('2')\n",
        "                print(lk, d)\n",
        "                df = pd.read_csv(os.path.join(load_path, f'{lk}_{d}'))\n",
        "                vocabs[lk][d] = dict(zip(*[df[c] for c in columns]))\n",
        "        return vocabs\n",
        "    \n",
        "    def save_vocabs(self, save_dir_path):\n",
        "        save_path = os.path.join(save_dir_path, 'vocabs')\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        for lk in self.lang_keys:\n",
        "            for d in self.directions:\n",
        "                columns = d.split('2')\n",
        "                pd.DataFrame(data=self.vocabs[lk][d].items(),\n",
        "                    columns=columns).to_csv(os.path.join(save_path, f'{lk}_{d}'),\n",
        "                                                index=False,\n",
        "                                                sep=',')\n",
        "    def make_vocabs(self, data_df):\n",
        "        for lk in self.lang_keys:\n",
        "            tokens = col.Counter(''.join(list(it.chain(*data_df[lk])))).keys()\n",
        "            part_id2t = dict(enumerate(tokens, start=len(self.service_token_names)))\n",
        "            part_t2id = {k:v for v,k in part_id2t.items()}\n",
        "            part_vocabs = [part_id2t, part_t2id]\n",
        "            for i in range(len(self.directions)):\n",
        "                self.vocabs[lk][self.directions[i]].update(part_vocabs[i])\n",
        "                \n",
        "        self.src_vocab_size = len(self.vocabs['en']['id2token'])\n",
        "        self.tgt_vocab_size = len(self.vocabs['ru']['id2token'])\n",
        "                \n",
        "    def frame(self, sample, start_token=None, end_token=None):\n",
        "        if start_token is None:\n",
        "            start_token=self.service_token_names['start_token']\n",
        "        if end_token is None:\n",
        "            end_token=self.service_token_names['end_token']\n",
        "        return [start_token] + sample + [end_token]\n",
        "    def token2id(self, samples, frame, lang_key):\n",
        "        if frame:\n",
        "            samples = list(map(self.frame, samples))\n",
        "        vocab = self.vocabs[lang_key]['token2id']\n",
        "        return list(map(lambda s:\n",
        "                        [vocab[t] if t in vocab.keys() else vocab[self.service_token_names['unk_token']]\n",
        "                         for t in s], samples))\n",
        "    \n",
        "    def unframe(self, sample, start_token=None, end_token=None):\n",
        "        if start_token is None:\n",
        "            start_token=self.service_vocabs['token2id'][self.service_token_names['start_token']]\n",
        "        if end_token is None:\n",
        "            end_token=self.service_vocabs['token2id'][self.service_token_names['end_token']]\n",
        "        pad_token=self.service_vocabs['token2id'][self.service_token_names['pad_token']]\n",
        "        return list(it.takewhile(lambda e: e != end_token and e != pad_token, sample[1:]))\n",
        "    def id2token(self, samples, unframe, lang_key):\n",
        "        if unframe:\n",
        "            samples = list(map(self.unframe, samples))\n",
        "        vocab = self.vocabs[lang_key]['id2token']\n",
        "        return list(map(lambda s:\n",
        "                        [vocab[idx] if idx in vocab.keys() else self.service_token_names['unk_token'] for idx in s], samples))\n",
        "\n",
        "\n",
        "class TranslitData(torch_data.Dataset):\n",
        "    def __init__(self, source_strings, target_strings,\n",
        "                text_encoder):\n",
        "        super(TranslitData, self).__init__()\n",
        "        self.source_strings = source_strings\n",
        "        self.text_encoder = text_encoder\n",
        "        if target_strings is not None:\n",
        "            assert len(source_strings) == len(target_strings)\n",
        "            self.target_strings = target_strings\n",
        "        else:\n",
        "            self.target_strings = None\n",
        "    def __len__(self):\n",
        "        return len(self.source_strings)\n",
        "    def __getitem__(self, idx):\n",
        "        src_str = self.source_strings[idx]\n",
        "        encoder_input = self.text_encoder.token2id([list(src_str)], frame=True, lang_key='en')[0]\n",
        "        if self.target_strings is not None:\n",
        "            tgt_str = self.target_strings[idx]\n",
        "            tmp = self.text_encoder.token2id([list(tgt_str)], frame=True, lang_key='ru')[0]\n",
        "            decoder_input = tmp[:-1]\n",
        "            decoder_target = tmp[1:]\n",
        "            return (encoder_input, decoder_input, decoder_target)\n",
        "        else:\n",
        "            return (encoder_input,)\n",
        "\n",
        "\n",
        "class BatchSampler(torch_data.BatchSampler):\n",
        "    def __init__(self, sampler, batch_size, drop_last, shuffle_each_epoch):\n",
        "        super(BatchSampler, self).__init__(sampler, batch_size, drop_last)\n",
        "        self.batches = []\n",
        "        for b in super(BatchSampler, self).__iter__():\n",
        "            self.batches.append(b)\n",
        "        self.shuffle_each_epoch = shuffle_each_epoch\n",
        "        if self.shuffle_each_epoch:\n",
        "            random.shuffle(self.batches)\n",
        "        self.index = 0\n",
        "        #print(f'Batches collected: {len(self.batches)}')\n",
        "    def __iter__(self):\n",
        "        self.index = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self.index == len(self.batches):\n",
        "            if self.shuffle_each_epoch:\n",
        "                random.shuffle(self.batches)\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            batch = self.batches[self.index]\n",
        "            self.index += 1\n",
        "            return batch\n",
        "\n",
        "def collate_fn(batch_list):\n",
        "    '''batch_list can store either 3 components:\n",
        "        encoder_inputs, decoder_inputs, decoder_targets\n",
        "        or single component: encoder_inputs'''\n",
        "    components = list(zip(*batch_list))\n",
        "    batch_tensors = []\n",
        "    for data in components:\n",
        "        max_len = max([len(sample) for sample in data])\n",
        "        #print(f'Maximum length in batch = {max_len}')\n",
        "        sample_tensors = [torch.tensor(s, requires_grad=False, dtype=torch.int64)\n",
        "                         for s in data]\n",
        "        batch_tensors.append(nn.utils.rnn.pad_sequence(\n",
        "            sample_tensors,\n",
        "            batch_first=True, padding_value=0))\n",
        "    return tuple(batch_tensors) \n",
        "\n",
        "\n",
        "def create_dataloader(source_strings, target_strings,\n",
        "                      text_encoder, batch_size,\n",
        "                      shuffle_batches_each_epoch):\n",
        "    '''target_strings parameter can be None'''\n",
        "    dataset = TranslitData(source_strings, target_strings,\n",
        "                                text_encoder=text_encoder)\n",
        "    seq_sampler = torch_data.SequentialSampler(dataset)\n",
        "    batch_sampler = BatchSampler(seq_sampler, batch_size=batch_size,\n",
        "                                drop_last=False,\n",
        "                                shuffle_each_epoch=shuffle_batches_each_epoch)\n",
        "    dataloader = torch_data.DataLoader(dataset,\n",
        "                                       batch_sampler=batch_sampler,\n",
        "                                       collate_fn=collate_fn)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "-gMDFNVt-nMw",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:33.944262Z",
          "iopub.execute_input": "2023-04-12T09:29:33.944949Z",
          "iopub.status.idle": "2023-04-12T09:29:33.980272Z",
          "shell.execute_reply.started": "2023-04-12T09:29:33.944894Z",
          "shell.execute_reply": "2023-04-12T09:29:33.978998Z"
        },
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric function"
      ],
      "metadata": {
        "id": "-Qsehb_eERRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(predicted_strings, target_strings, metrics):\n",
        "    metric_values = {}\n",
        "    for m in metrics:\n",
        "        if m == 'acc@1':\n",
        "            metric_values[m] = sum(predicted_strings == target_strings) / len(target_strings)\n",
        "        elif m =='mean_ld@1':\n",
        "            metric_values[m] =\\\n",
        "                np.mean(list(map(lambda e: le.distance(*e), zip(predicted_strings, target_strings))))\n",
        "        else: \n",
        "            raise ValueError(f'Unknown metric: {m}')\n",
        "    return metric_values"
      ],
      "metadata": {
        "id": "MN4I60GqETB0",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:33.982024Z",
          "iopub.execute_input": "2023-04-12T09:29:33.982459Z",
          "iopub.status.idle": "2023-04-12T09:29:33.993490Z",
          "shell.execute_reply.started": "2023-04-12T09:29:33.982417Z",
          "shell.execute_reply": "2023-04-12T09:29:33.992458Z"
        },
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Positional Encoding"
      ],
      "metadata": {
        "id": "UiYl5XsdkmZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you remember, Transformer treats an input sequence of elements as a time series. Since the Encoder inside the Transformer simultaneously processes the entire input sequence, the information about the position of the element needs to be encoded inside its embedding, since it is not identified in any other way inside the model. That is why the PositionalEncoding layer is used, which sums embeddings with a vector of the same dimension.\n",
        "Let the matrix of these vectors for each position of the time series be denoted as $PE$. Then the elements of the matrix are:\n",
        "\n",
        "$$ PE_{(pos,2i)} = \\sin{(pos/10000^{2i/d_{model}})}$$\n",
        "$$ PE_{(pos,2i+1)} = \\cos{(pos/10000^{2i/d_{model}})}$$\n",
        "\n",
        "where $pos$ - is the position, $i$ - index of the component of the corresponging vector, $d_{model}$ - dimension of each vector. Thus, even components represent sine values, and odd ones represent cosine values with different arguments.\n",
        "\n",
        "In this task you are required to implement these formulas inside the class constructor *PositionalEncoding* in the main file ``translit.py``, which you are to upload. To run the test use the following function:\n",
        "\n",
        "`test_positional_encoding()`\n",
        "\n",
        "Make sure that there is no any `AssertionError`!\n"
      ],
      "metadata": {
        "id": "vkNaSzwrkpf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.emb_layer = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb_layer(x)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_size, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, hidden_size, requires_grad=False)\n",
        "        # TODO: implement your code here\n",
        "        position = torch.arange(max_len)[:, None]\n",
        "        div_term = torch.exp(torch.arange(0, hidden_size, 2) * -(np.log(10000.0) / hidden_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        # pe shape: (1, max_len, hidden_size)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: shape (batch size, sequence length, hidden size)\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "Rm4g1vybAKZs",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:33.994970Z",
          "iopub.execute_input": "2023-04-12T09:29:33.995530Z",
          "iopub.status.idle": "2023-04-12T09:29:34.006758Z",
          "shell.execute_reply.started": "2023-04-12T09:29:33.995493Z",
          "shell.execute_reply": "2023-04-12T09:29:34.005704Z"
        },
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positional_encoding():\n",
        "    pe = PositionalEncoding(max_len=3, hidden_size=4)\n",
        "    res_1 = torch.tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
        "                           [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
        "                           [ 0.9093, -0.4161,  0.0200,  0.9998]]])\n",
        "    # print(pe.pe - res_1)\n",
        "    assert torch.all(torch.abs(pe.pe - res_1) < 1e-4).item()\n",
        "    print('Test is passed!')"
      ],
      "metadata": {
        "id": "hBk6uxQyCqt-",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.008527Z",
          "iopub.execute_input": "2023-04-12T09:29:34.008896Z",
          "iopub.status.idle": "2023-04-12T09:29:34.020672Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.008860Z",
          "shell.execute_reply": "2023-04-12T09:29:34.019561Z"
        },
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_positional_encoding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwN9Qk_NCw-K",
        "outputId": "00e5f5d3-3e7d-495b-feb9-32b048ed751d",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.022407Z",
          "iopub.execute_input": "2023-04-12T09:29:34.022776Z",
          "iopub.status.idle": "2023-04-12T09:29:34.150445Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.022740Z",
          "shell.execute_reply": "2023-04-12T09:29:34.149301Z"
        },
        "trusted": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test is passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LayerNorm"
      ],
      "metadata": {
        "id": "g026bdkrEtiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Layer Normalization layer\"\n",
        "\n",
        "    def __init__(self, hidden_size, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gain = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gain * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "dIMy52O9Es0K",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.154619Z",
          "iopub.execute_input": "2023-04-12T09:29:34.155705Z",
          "iopub.status.idle": "2023-04-12T09:29:34.163053Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.155659Z",
          "shell.execute_reply": "2023-04-12T09:29:34.162008Z"
        },
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SublayerConnection"
      ],
      "metadata": {
        "id": "TEpsKqLwE3hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.layer_norm = LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return self.layer_norm(x + self.dropout(sublayer(x)))\n",
        "\n",
        "def padding_mask(x, pad_idx=0):\n",
        "    assert len(x.size()) >= 2\n",
        "    return (x != pad_idx).unsqueeze(-2)\n",
        "\n",
        "def look_ahead_mask(size):\n",
        "    \"Mask out the right context\"\n",
        "    attn_shape = (1, size, size)\n",
        "    look_ahead_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(look_ahead_mask) == 0\n",
        "\n",
        "def compositional_mask(x, pad_idx=0):\n",
        "    pm = padding_mask(x, pad_idx=pad_idx)\n",
        "    seq_length = x.size(-1)\n",
        "    result_mask = pm & \\\n",
        "                  look_ahead_mask(seq_length).type_as(pm.data)\n",
        "    return result_mask"
      ],
      "metadata": {
        "id": "2yuMxRinE3uH",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.164571Z",
          "iopub.execute_input": "2023-04-12T09:29:34.165049Z",
          "iopub.status.idle": "2023-04-12T09:29:34.176897Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.165007Z",
          "shell.execute_reply": "2023-04-12T09:29:34.175828Z"
        },
        "trusted": true
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FeedForward"
      ],
      "metadata": {
        "id": "Kh86csH_FCyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hidden_size, ff_hidden_size, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.pre_linear = nn.Linear(hidden_size, ff_hidden_size)\n",
        "        self.post_linear = nn.Linear(ff_hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.post_linear(self.dropout(F.relu(self.pre_linear(x))))\n",
        "\n",
        "def clone_layer(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "4zy1PNwlGIEX",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.178439Z",
          "iopub.execute_input": "2023-04-12T09:29:34.179993Z",
          "iopub.status.idle": "2023-04-12T09:29:34.190264Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.179947Z",
          "shell.execute_reply": "2023-04-12T09:29:34.189229Z"
        },
        "trusted": true
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  MultiHeadAttention"
      ],
      "metadata": {
        "id": "IwoQ_X8ylJYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Then you are required to implement `attention` method in the class  `MultiHeadAttention`. The MultiHeadAttention layer takes as input  query vectors, key and value vectors for each step of the sequence of matrices  Q,K,V correspondingly. Each key vector, value vector, and query vector is obtained as a result of linear projection using one of three trained vector parameter matrices from the previous layer. This semantics can be represented in the form of formulas:\n",
        "$$\n",
        "Attention(Q, K, V)=softmax\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\\\\\n",
        "$$\n",
        "\n",
        "$$\n",
        "MultiHead(Q, K, V) = Concat\\left(head_1, ... , head_h\\right) W^O\\\\\n",
        "$$\n",
        "\n",
        "$$\n",
        "head_i=Attention\\left(Q W_i^Q, K W_i^K, V W_i^V\\right)\\\\\n",
        "$$\n",
        "$h$ - the number of attention heads - parallel sub-layers for Scaled Dot-Product Attention on a vector of smaller dimension ($d_{k} = d_{q} = d_{v} = d_{model} / h$). \n",
        "The logic of  \\texttt{MultiHeadAttention} is presented in the picture (from original  [paper](https://arxiv.org/abs/1706.03762)):\n",
        "\n",
        "![](https://lilianweng.github.io/lil-log/assets/images/transformer.png)\n",
        "\n",
        "\n",
        "Inside a method `attention` you are required to create a dropout layer from  MultiHeadAttention class constructor. Dropout layer is to be applied directly on the attention weights - the result of softmax operation. Value of drop probability  can be regulated in the train in the `model_config['dropout']['attention']`.\n",
        "\n",
        "The correctness of implementation can be checked with\n",
        "`test_multi_head_attention()`\n",
        "\n"
      ],
      "metadata": {
        "id": "SYGVEp3mkgNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, hidden_size, dropout=None):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert hidden_size % n_heads == 0\n",
        "        self.head_hidden_size = hidden_size // n_heads\n",
        "        self.n_heads = n_heads\n",
        "        self.linears = clone_layer(nn.Linear(hidden_size, hidden_size), 4)\n",
        "        self.attn_weights = None\n",
        "        self.dropout = dropout\n",
        "        if self.dropout is not None:\n",
        "            self.dropout_layer = nn.Dropout(p=self.dropout)\n",
        "\n",
        "    def attention(self, query, key, value, mask):\n",
        "        \"\"\"Compute 'Scaled Dot Product Attention'\n",
        "            query, key and value tensors have the same shape:\n",
        "                (batch size, number of heads, sequence length, head hidden size)\n",
        "            mask shape: (batch size, 1, sequence length, sequence length)\n",
        "                '1' dimension value will be broadcasted to number of heads inside your operations\n",
        "            mask should be applied before using softmax to get attn_weights\n",
        "        \"\"\"\n",
        "        ## attn_weights shape: (batch size, number of heads, sequence length, sequence length)\n",
        "        ## output shape: (batch size, number of heads, sequence length, head hidden size)\n",
        "        ## TODO: provide your implementation here\n",
        "        ## don't forget to apply dropout to attn_weights if self.dropout is not None\n",
        "        d = query.shape[-1]\n",
        "        scores = query @ key.transpose(-2, -1) / np.sqrt(d)\n",
        "\n",
        "        if mask is not None:\n",
        "          scores = scores.masked_fill(mask==0, -1e9)\n",
        "        \n",
        "        attn_weights = F.softmax(scores, -1)\n",
        "        if self.dropout is not None:\n",
        "            attn_weights = self.dropout_layer(attn_weights)\n",
        "\n",
        "        output = attn_weights @ value\n",
        "        #raise NotImplementedError\n",
        "        return output, attn_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Split vectors for different attention heads (from hidden_size => n_heads x head_hidden_size)\n",
        "        # and do separate linear projection, for separate trainable weights\n",
        "        query, key, value = \\\n",
        "            [l(x).view(batch_size, -1, self.n_heads, self.head_hidden_size).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "        x, self.attn_weights = self.attention(query, key, value, mask=mask)\n",
        "        # x shape: (batch size, number of heads, sequence length, head hidden size)\n",
        "        # self.attn_weights shape: (batch size, number of heads, sequence length, sequence length)\n",
        "\n",
        "        # Concatenate the output of each head\n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "            .view(batch_size, -1, self.n_heads * self.head_hidden_size)\n",
        "\n",
        "        return self.linears[-1](x)"
      ],
      "metadata": {
        "id": "5q7mpdjnAVHP",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.193680Z",
          "iopub.execute_input": "2023-04-12T09:29:34.193998Z",
          "iopub.status.idle": "2023-04-12T09:29:34.207992Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.193967Z",
          "shell.execute_reply": "2023-04-12T09:29:34.206611Z"
        },
        "trusted": true
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_head_attention():\n",
        "    mha = MultiHeadAttention(n_heads=1, hidden_size=5, dropout=None)\n",
        "    # batch_size == 2, sequence length == 3, hidden_size == 5\n",
        "    # query = torch.arange(150).reshape(2, 3, 5)\n",
        "    query = torch.tensor([[[[ 0.64144618, -0.95817388,  0.37432297,  0.58427106,\n",
        "          -0.94668716]],\n",
        "        [[-0.23199289,  0.66329209, -0.46507035, -0.54272512,\n",
        "          -0.98640698]],\n",
        "        [[ 0.07546638, -0.09277002,  0.20107185, -0.97407381,\n",
        "          -0.27713414]]],\n",
        "       [[[ 0.14727783,  0.4747886 ,  0.44992016, -0.2841419 ,\n",
        "          -0.81820319]],\n",
        "        [[-0.72324994,  0.80643179, -0.47655449,  0.45627872,\n",
        "           0.60942404]],\n",
        "        [[ 0.61712569, -0.62947282, -0.95215713, -0.38721959,\n",
        "          -0.73289725]]]])\n",
        "    key = torch.tensor([[[[-0.81759856, -0.60049991, -0.05923424,  0.51898901,\n",
        "          -0.3366209 ]],\n",
        "        [[ 0.83957818, -0.96361722,  0.62285191,  0.93452467,\n",
        "           0.51219613]],\n",
        "        [[-0.72758847,  0.41256154,  0.00490795,  0.59892503,\n",
        "          -0.07202049]]],\n",
        "       [[[ 0.72315339, -0.49896314,  0.94254637, -0.54356006,\n",
        "          -0.04837949]],\n",
        "        [[ 0.51759322, -0.43927061, -0.59924184,  0.92241702,\n",
        "          -0.86811696]],\n",
        "        [[-0.54322046, -0.92323003, -0.827746  ,  0.90842783,\n",
        "           0.88428119]]]])\n",
        "    value = torch.tensor([[[[-0.83895431,  0.805027  ,  0.22298283, -0.84849915,\n",
        "          -0.34906026]],\n",
        "        [[-0.02899652, -0.17456128, -0.17535998, -0.73160314,\n",
        "          -0.13468061]],\n",
        "        [[ 0.75234265,  0.02675947,  0.84766286, -0.5475651 ,\n",
        "          -0.83319316]]],\n",
        "       [[[-0.47834413,  0.34464645, -0.41921457,  0.33867964,\n",
        "           0.43470836]],\n",
        "        [[-0.99000979,  0.10220893, -0.4932273 ,  0.95938905,\n",
        "           0.01927012]],\n",
        "        [[ 0.91607137,  0.57395644, -0.90914179,  0.97212912,\n",
        "           0.33078759]]]])\n",
        "    query = query.float().transpose(1,2)\n",
        "    key = key.float().transpose(1,2)\n",
        "    value = value.float().transpose(1,2)\n",
        "\n",
        "    x,_ = torch.max(query[:,0,:,:], axis=-1)\n",
        "    mask = compositional_mask(x)\n",
        "    mask.unsqueeze_(1)\n",
        "    for n,t in [('query', query), ('key', key), ('value', value), ('mask', mask)]:\n",
        "        print(f'Name: {n}, shape: {t.size()}')\n",
        "    with torch.no_grad():\n",
        "        output, attn_weights = mha.attention(query, key, value, mask=mask)\n",
        "    assert output.size() == torch.Size([2,1,3,5])\n",
        "    assert attn_weights.size() == torch.Size([2,1,3,3])\n",
        "\n",
        "    truth_output = torch.tensor([[[[-0.8390,  0.8050,  0.2230, -0.8485, -0.3491],\n",
        "          [-0.6043,  0.5212,  0.1076, -0.8146, -0.2870],\n",
        "          [-0.0665,  0.2461,  0.3038, -0.7137, -0.4410]]],\n",
        "        [[[-0.4783,  0.3446, -0.4192,  0.3387,  0.4347],\n",
        "          [-0.7959,  0.1942, -0.4652,  0.7239,  0.1769],\n",
        "          [-0.3678,  0.2868, -0.5799,  0.7987,  0.2086]]]])\n",
        "    truth_attn_weights = torch.tensor([[[[1.0000, 0.0000, 0.0000],\n",
        "          [0.7103, 0.2897, 0.0000],\n",
        "          [0.3621, 0.3105, 0.3274]]],\n",
        "        [[[1.0000, 0.0000, 0.0000],\n",
        "          [0.3793, 0.6207, 0.0000],\n",
        "          [0.2642, 0.4803, 0.2555]]]])\n",
        "    # print(torch.abs(output - truth_output))\n",
        "    # print(torch.abs(attn_weights - truth_attn_weights))\n",
        "    assert torch.all(torch.abs(output - truth_output) < 1e-4).item()\n",
        "    assert torch.all(torch.abs(attn_weights - truth_attn_weights) < 1e-4).item()\n",
        "    print('Test is passed!')"
      ],
      "metadata": {
        "id": "ExHkza22FCF0",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.209708Z",
          "iopub.execute_input": "2023-04-12T09:29:34.210677Z",
          "iopub.status.idle": "2023-04-12T09:29:34.230282Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.210635Z",
          "shell.execute_reply": "2023-04-12T09:29:34.229215Z"
        },
        "trusted": true
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_multi_head_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI6yMbX8FhGZ",
        "outputId": "9ca95252-791e-44c8-9e27-0b62e86d6b16",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.231804Z",
          "iopub.execute_input": "2023-04-12T09:29:34.232438Z",
          "iopub.status.idle": "2023-04-12T09:29:34.279947Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.232400Z",
          "shell.execute_reply": "2023-04-12T09:29:34.278709Z"
        },
        "trusted": true
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: query, shape: torch.Size([2, 1, 3, 5])\n",
            "Name: key, shape: torch.Size([2, 1, 3, 5])\n",
            "Name: value, shape: torch.Size([2, 1, 3, 5])\n",
            "Name: mask, shape: torch.Size([2, 1, 3, 3])\n",
            "Test is passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "LUdLLSojGJbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "\n",
        "    def __init__(self, hidden_size, ff_hidden_size, n_heads, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(n_heads, hidden_size,\n",
        "                                            dropout=dropout['attention'])\n",
        "        self.feed_forward = FeedForward(hidden_size, ff_hidden_size,\n",
        "                                        dropout=dropout['relu'])\n",
        "        self.sublayers = clone_layer(SublayerConnection(hidden_size, dropout['residual']), 2)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayers[1](x, self.feed_forward)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedder = Embedding(config['hidden_size'],\n",
        "                                  config['src_vocab_size'])\n",
        "        self.positional_encoder = PositionalEncoding(config['hidden_size'],\n",
        "                                                     max_len=config['max_src_seq_length'])\n",
        "        self.embedding_dropout = nn.Dropout(p=config['dropout']['embedding'])\n",
        "        self.encoder_layer = EncoderLayer(config['hidden_size'],\n",
        "                                          config['ff_hidden_size'],\n",
        "                                          config['n_heads'],\n",
        "                                          config['dropout'])\n",
        "        self.layers = clone_layer(self.encoder_layer, config['n_layers'])\n",
        "        self.layer_norm = LayerNorm(config['hidden_size'])\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        x = self.embedding_dropout(self.positional_encoder(self.embedder(x)))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.layer_norm(x)"
      ],
      "metadata": {
        "id": "dFMMTX4NA0KP",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.281739Z",
          "iopub.execute_input": "2023-04-12T09:29:34.282447Z",
          "iopub.status.idle": "2023-04-12T09:29:34.294032Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.282407Z",
          "shell.execute_reply": "2023-04-12T09:29:34.292889Z"
        },
        "trusted": true
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "6kyeSkMeGQo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder is made of 3 sublayers: self attention, encoder-decoder attention\n",
        "    and feed forward\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, ff_hidden_size, n_heads, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(n_heads, hidden_size,\n",
        "                                            dropout=dropout['attention'])\n",
        "        self.encdec_attn = MultiHeadAttention(n_heads, hidden_size,\n",
        "                                              dropout=dropout['attention'])\n",
        "        self.feed_forward = FeedForward(hidden_size, ff_hidden_size,\n",
        "                                        dropout=dropout['relu'])\n",
        "        self.sublayers = clone_layer(SublayerConnection(hidden_size, dropout['residual']), 3)\n",
        "\n",
        "    def forward(self, x, encoder_output, encoder_mask, decoder_mask):\n",
        "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, decoder_mask))\n",
        "        x = self.sublayers[1](x, lambda x: self.encdec_attn(x, encoder_output,\n",
        "                                                            encoder_output, encoder_mask))\n",
        "        return self.sublayers[2](x, self.feed_forward)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedder = Embedding(config['hidden_size'],\n",
        "                                  config['tgt_vocab_size'])\n",
        "        self.positional_encoder = PositionalEncoding(config['hidden_size'],\n",
        "                                                     max_len=config['max_tgt_seq_length'])\n",
        "        self.embedding_dropout = nn.Dropout(p=config['dropout']['embedding'])\n",
        "        self.decoder_layer = DecoderLayer(config['hidden_size'],\n",
        "                                          config['ff_hidden_size'],\n",
        "                                          config['n_heads'],\n",
        "                                          config['dropout'])\n",
        "        self.layers = clone_layer(self.decoder_layer, config['n_layers'])\n",
        "        self.layer_norm = LayerNorm(config['hidden_size'])\n",
        "\n",
        "    def forward(self, x, encoder_output, encoder_mask, decoder_mask):\n",
        "        x = self.embedding_dropout(self.positional_encoder(self.embedder(x)))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, encoder_mask, decoder_mask)\n",
        "        return self.layer_norm(x)"
      ],
      "metadata": {
        "id": "B4pSnS8NGPyf",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.296843Z",
          "iopub.execute_input": "2023-04-12T09:29:34.297228Z",
          "iopub.status.idle": "2023-04-12T09:29:34.310743Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.297190Z",
          "shell.execute_reply": "2023-04-12T09:29:34.309655Z"
        },
        "trusted": true
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "bwP_NVeYGY-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.config = config\n",
        "        self.encoder = Encoder(config)\n",
        "        self.decoder = Decoder(config)\n",
        "        self.proj = nn.Linear(config['hidden_size'], config['tgt_vocab_size'])\n",
        "\n",
        "        self.pad_idx = config['pad_idx']\n",
        "        self.tgt_vocab_size = config['tgt_vocab_size']\n",
        "\n",
        "    def encode(self, encoder_input, encoder_input_mask):\n",
        "        return self.encoder(encoder_input, encoder_input_mask)\n",
        "\n",
        "    def decode(self, encoder_output, encoder_input_mask, decoder_input, decoder_input_mask):\n",
        "        return self.decoder(decoder_input, encoder_output, encoder_input_mask, decoder_input_mask)\n",
        "\n",
        "    def linear_project(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        encoder_input_mask = padding_mask(encoder_input, pad_idx=self.config['pad_idx'])\n",
        "        decoder_input_mask = compositional_mask(decoder_input, pad_idx=self.config['pad_idx'])\n",
        "        encoder_output = self.encode(encoder_input, encoder_input_mask)\n",
        "        decoder_output = self.decode(encoder_output, encoder_input_mask,\n",
        "                                     decoder_input, decoder_input_mask)\n",
        "        output_logits = self.linear_project(decoder_output)\n",
        "        return output_logits\n",
        "\n",
        "\n",
        "def prepare_model(config):\n",
        "    model = Transformer(config)\n",
        "\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "metadata": {
        "id": "mVTRK_5cGYYa",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.312269Z",
          "iopub.execute_input": "2023-04-12T09:29:34.312875Z",
          "iopub.status.idle": "2023-04-12T09:29:34.325557Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.312838Z",
          "shell.execute_reply": "2023-04-12T09:29:34.324474Z"
        },
        "trusted": true
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  LrScheduler"
      ],
      "metadata": {
        "id": "wnmPBcVyrR6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last thing you have to prepare is the class  `LrScheduler`, which is in charge of  learning rate updating after every step of the optimizer. You are required to fill the class constructor and the method `learning_rate`. The preferable stratagy of updating the learning rate (lr), is the following two stages:\n",
        "\n",
        "* \"warmup\" stage - lr linearly increases until the defined value during the fixed number of steps (the proportion of all training steps - the parameter `train_config['warmup\\_steps\\_part']` in the train function). \n",
        "* \"decrease\" stage - lr linearly decreases until 0 during the left training steps.\n",
        "\n",
        "`learning_rate()` call should return the value of  lr at this step,  which number is stored at self.step. The class constructor takes not only `warmup_steps_part` but the peak learning rate value `lr_peak` at the end of \"warmup\" stage and a string name of the strategy of learning rate scheduling. You can test other strategies if you want to with `self.type attribute`. \n",
        "\n",
        "Correctness check: `test_lr_scheduler()`\n"
      ],
      "metadata": {
        "id": "2luuBDZFrTj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LrScheduler:\n",
        "    def __init__(self, n_steps, **kwargs):\n",
        "        self.type = kwargs['type']\n",
        "        if self.type == 'warmup,decay_linear':\n",
        "            ## TODO: provide your implementation here\n",
        "            self.n_steps = n_steps\n",
        "            self.warmup_steps_part = kwargs['warmup_steps_part']\n",
        "            self.lr_peak = kwargs['lr_peak']\n",
        "            #raise NotImplementedError\n",
        "        else:\n",
        "            raise ValueError(f'Unknown type argument: {self.type}')\n",
        "        self._step = 0\n",
        "        self._lr = 0\n",
        "\n",
        "    def step(self, optimizer):\n",
        "        self._step += 1\n",
        "        lr = self.learning_rate()\n",
        "        for p in optimizer.param_groups:\n",
        "            p['lr'] = lr\n",
        "\n",
        "    def learning_rate(self, step=None):\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        if self.type == 'warmup,decay_linear':\n",
        "            ## TODO: provide your implementation here\n",
        "            if step <= self.warmup_steps_part * self.n_steps:\n",
        "              self._lr = self.lr_peak * step / (self.warmup_steps_part * self.n_steps)\n",
        "            else:\n",
        "              self._lr = self.lr_peak * (1 - step / self.n_steps) / (1 - self.warmup_steps_part)\n",
        "            #raise NotImplementedError\n",
        "        return self._lr\n",
        "    \n",
        "\n",
        "    def state_dict(self):\n",
        "        sd = copy.deepcopy(self.__dict__)\n",
        "        return sd\n",
        "\n",
        "    def load_state_dict(self, sd):\n",
        "        for k in sd.keys():\n",
        "            self.__setattr__(k, sd[k])"
      ],
      "metadata": {
        "id": "YDvKYF5EAdnX",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.327373Z",
          "iopub.execute_input": "2023-04-12T09:29:34.327870Z",
          "iopub.status.idle": "2023-04-12T09:29:34.338804Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.327825Z",
          "shell.execute_reply": "2023-04-12T09:29:34.337551Z"
        },
        "trusted": true
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lr_scheduler():\n",
        "    lrs_type = 'warmup,decay_linear'\n",
        "    warmup_steps_part =  0.1\n",
        "    lr_peak = 3e-4\n",
        "    sch = LrScheduler(100, type=lrs_type, warmup_steps_part=warmup_steps_part,\n",
        "                      lr_peak=lr_peak)\n",
        "    assert sch.learning_rate(step=5) - 15e-5 < 1e-6\n",
        "    assert sch.learning_rate(step=10) - 3e-4 < 1e-6\n",
        "    assert sch.learning_rate(step=50) - 166e-6 < 1e-6\n",
        "    assert sch.learning_rate(step=100) - 0. < 1e-6\n",
        "    print('Test is passed!')"
      ],
      "metadata": {
        "id": "4JHOgJDBGjhr",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.340623Z",
          "iopub.execute_input": "2023-04-12T09:29:34.341069Z",
          "iopub.status.idle": "2023-04-12T09:29:34.353513Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.341033Z",
          "shell.execute_reply": "2023-04-12T09:29:34.352458Z"
        },
        "trusted": true
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lr_scheduler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Ys4DZRGmzM",
        "outputId": "79d1f0bf-caa1-404e-e3f4-b4ec1037b38c",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:29:34.355040Z",
          "iopub.execute_input": "2023-04-12T09:29:34.356047Z",
          "iopub.status.idle": "2023-04-12T09:29:34.365901Z",
          "shell.execute_reply.started": "2023-04-12T09:29:34.356008Z",
          "shell.execute_reply": "2023-04-12T09:29:34.364423Z"
        },
        "trusted": true
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test is passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run and translate"
      ],
      "metadata": {
        "id": "byCY6Tn-A9i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "def run_epoch(data_iter, model, lr_scheduler, optimizer, device, label_smoothing=0, verbose=False, print_info=False):\n",
        "    start = time.time()\n",
        "    local_start = start\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    loss_fn = nn.CrossEntropyLoss(reduction='sum', label_smoothing=label_smoothing)\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        encoder_input = batch[0].to(device)\n",
        "        decoder_input = batch[1].to(device)\n",
        "        decoder_target = batch[2].to(device)\n",
        "        logits = model(encoder_input, decoder_input)\n",
        "        loss = loss_fn(logits.view(-1, model.tgt_vocab_size),\n",
        "                       decoder_target.view(-1))\n",
        "        total_loss += loss.item()\n",
        "        batch_n_tokens = (decoder_target != model.pad_idx).sum().item()\n",
        "        total_tokens += batch_n_tokens\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            lr_scheduler.step(optimizer)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        tokens += batch_n_tokens\n",
        "        if verbose and i % 1000 == 1:\n",
        "            elapsed = time.time() - local_start\n",
        "            print(\"batch number: %d, accumulated average loss: %f, tokens per second: %f\" %\n",
        "                  (i, total_loss / total_tokens, tokens / elapsed))\n",
        "            local_start = time.time()\n",
        "            tokens = 0\n",
        "\n",
        "    average_loss = total_loss / total_tokens\n",
        "    epoch_elapsed_time = format_time(time.time() - start)\n",
        "    if print_info:\n",
        "        print('** End of epoch, accumulated average loss = %f **' % average_loss)\n",
        "        print(f'** Elapsed time: {epoch_elapsed_time}**')\n",
        "    return average_loss\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch, model, lr_scheduler, optimizer, model_dir_path):\n",
        "    save_path = os.path.join(model_dir_path, f'cpkt_{epoch}_epoch')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'lr_scheduler_state_dict': lr_scheduler.state_dict()\n",
        "    }, save_path)\n",
        "    print(f'Saved checkpoint to {save_path}')\n",
        "\n",
        "def load_model(epoch, model_dir_path):\n",
        "    save_path = os.path.join(model_dir_path, f'cpkt_{epoch}_epoch')\n",
        "    checkpoint = torch.load(save_path)\n",
        "    with open(os.path.join(model_dir_path, 'model_config.json'), 'r', encoding='utf-8') as rf:\n",
        "        model_config = json.load(rf)\n",
        "    model = prepare_model(model_config)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model\n",
        "\n",
        "def greedy_decode(model, device, encoder_input, max_len, start_symbol):\n",
        "    batch_size = encoder_input.size()[0]\n",
        "    decoder_input = torch.ones(batch_size, 1).fill_(start_symbol).type_as(encoder_input.data).to(device)\n",
        "\n",
        "    for i in range(max_len):\n",
        "        logits = model(encoder_input, decoder_input)\n",
        "\n",
        "        _, predicted_ids = torch.max(logits, dim=-1)\n",
        "        next_word = predicted_ids[:, i]\n",
        "        # print(next_word)\n",
        "        rest = torch.ones(batch_size, 1).type_as(decoder_input.data)\n",
        "        # print(rest[:,0].size(), next_word.size())\n",
        "        rest[:, 0] = next_word\n",
        "        decoder_input = torch.cat([decoder_input, rest], dim=1).to(device)\n",
        "        # print(decoder_input)\n",
        "    return decoder_input\n",
        "\n",
        "def generate_predictions(dataloader, max_decoding_len, text_encoder, model, device):\n",
        "    # print(f'Max decoding length = {max_decoding_len}')\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    start_token_id = text_encoder.service_vocabs['token2id'][\n",
        "        text_encoder.service_token_names['start_token']]\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            encoder_input = batch[0].to(device)\n",
        "            prediction_tensor = \\\n",
        "                greedy_decode(model, device, encoder_input, max_decoding_len,\n",
        "                              start_token_id)\n",
        "\n",
        "            predictions.extend([''.join(e) for e in text_encoder.id2token(prediction_tensor.cpu().numpy(),\n",
        "                                                                          unframe=True, lang_key='ru')])\n",
        "    return np.array(predictions)\n",
        "\n",
        "\n",
        "def train(source_strings, target_strings, n_epochs=100, dropout=0.1, lr_peak=3e-4, label_smoothing=0):\n",
        "    '''Common training cycle for final run (fixed hyperparameters,\n",
        "    no evaluation during training)'''\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f'Using GPU device: {device}')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(f'GPU is not available, using CPU device {device}')\n",
        "\n",
        "    train_df = pd.DataFrame({'en': source_strings, 'ru': target_strings})\n",
        "    text_encoder = TextEncoder()\n",
        "    text_encoder.make_vocabs(train_df)\n",
        "\n",
        "    model_config = {\n",
        "        'src_vocab_size': text_encoder.src_vocab_size,\n",
        "        'tgt_vocab_size': text_encoder.tgt_vocab_size,\n",
        "        'max_src_seq_length': max(train_df['en'].aggregate(len)) + 2, #including start_token and end_token\n",
        "        'max_tgt_seq_length': max(train_df['ru'].aggregate(len)) + 2,\n",
        "        'n_layers': 2,\n",
        "        'n_heads': 2,\n",
        "        'hidden_size': 128,\n",
        "        'ff_hidden_size': 256,\n",
        "        'dropout': {\n",
        "            'embedding': dropout,\n",
        "            'attention': dropout,\n",
        "            'residual': dropout,\n",
        "            'relu': dropout\n",
        "        },\n",
        "        'pad_idx': 0\n",
        "    }\n",
        "    model = prepare_model(model_config)\n",
        "    model.to(device)\n",
        "\n",
        "    train_config = {'batch_size': 200, 'n_epochs': n_epochs, 'lr_scheduler': {\n",
        "        'type': 'warmup,decay_linear',\n",
        "        'warmup_steps_part': 0.1,\n",
        "        'lr_peak': 3e-4,\n",
        "    }}\n",
        "\n",
        "    #Model training procedure\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.)\n",
        "    n_steps = (len(train_df) // train_config['batch_size'] + 1) * train_config['n_epochs']\n",
        "    lr_scheduler = LrScheduler(n_steps, **train_config['lr_scheduler'])\n",
        "\n",
        "    # prepare train data\n",
        "    source_strings, target_strings = zip(*sorted(zip(source_strings, target_strings),\n",
        "                                                 key=lambda e: len(e[0])))\n",
        "    train_dataloader = create_dataloader(source_strings, target_strings, text_encoder,\n",
        "                                         train_config['batch_size'],\n",
        "                                         shuffle_batches_each_epoch=True)\n",
        "    # training cycle\n",
        "    for epoch in range(1,train_config['n_epochs']+1):\n",
        "        if (epoch - 1)%20 == 0:\n",
        "            print('\\n' + '-'*40)\n",
        "            print(f'Epoch: {epoch}')\n",
        "            print(f'Run training...')\n",
        "        model.train()\n",
        "        run_epoch(train_dataloader, model,\n",
        "                  lr_scheduler, optimizer,\n",
        "                  device=device, label_smoothing=label_smoothing,\n",
        "                  verbose=False,\n",
        "                  print_info=((epoch-1)%20==0))\n",
        "    learnable_params = {\n",
        "        'model': model,\n",
        "        'text_encoder': text_encoder,\n",
        "    }\n",
        "    return learnable_params\n",
        "\n",
        "def classify(source_strings, learnable_params):\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f'Using GPU device: {device}')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(f'GPU is not available, using CPU device {device}')\n",
        "\n",
        "    model = learnable_params['model']\n",
        "    text_encoder = learnable_params['text_encoder']\n",
        "    batch_size = 200\n",
        "    dataloader = create_dataloader(source_strings, None, text_encoder,\n",
        "                                   batch_size, shuffle_batches_each_epoch=False)\n",
        "    max_decoding_len = model.config['max_tgt_seq_length']\n",
        "    predictions = generate_predictions(dataloader, max_decoding_len, text_encoder, model, device)\n",
        "    #return single top1 prediction for each sample\n",
        "    return np.expand_dims(predictions, 1)"
      ],
      "metadata": {
        "id": "-K7-KJEGA8po",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:30:38.686903Z",
          "iopub.execute_input": "2023-04-12T09:30:38.687318Z",
          "iopub.status.idle": "2023-04-12T09:30:38.719502Z",
          "shell.execute_reply.started": "2023-04-12T09:30:38.687284Z",
          "shell.execute_reply": "2023-04-12T09:30:38.718329Z"
        },
        "trusted": true
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "jpG6i8X-HMmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PREDS_FNAME = \"preds_translit.tsv\"\n",
        "SCORED_PARTS = ('train', 'dev', 'train_small', 'dev_small', 'test')\n",
        "TRANSLIT_PATH = \"TRANSLIT\""
      ],
      "metadata": {
        "id": "f-7-YtzEKnug",
        "execution": {
          "iopub.status.busy": "2023-04-12T09:30:41.343388Z",
          "iopub.execute_input": "2023-04-12T09:30:41.344027Z",
          "iopub.status.idle": "2023-04-12T09:30:41.349076Z",
          "shell.execute_reply.started": "2023-04-12T09:30:41.343992Z",
          "shell.execute_reply": "2023-04-12T09:30:41.347998Z"
        },
        "trusted": true
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 1\n",
        "part2ixy = load_dataset(TRANSLIT_PATH, parts=SCORED_PARTS)\n",
        "train_ids, train_strings, train_transliterations = part2ixy['train']\n",
        "print('\\nTraining classifier on %d examples from train set ...' % len(train_strings))\n",
        "st = time.time()\n",
        "params = train(train_strings, train_transliterations)\n",
        "print('Classifier trained in %.2fs' % (time.time() - st))"
      ],
      "metadata": {
        "id": "74GcLUTuLFyS",
        "outputId": "7852e2ac-0fd5-420a-91d7-638cad160f06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2023-04-11T11:23:29.201104Z",
          "iopub.execute_input": "2023-04-11T11:23:29.202058Z",
          "iopub.status.idle": "2023-04-11T11:50:13.114212Z",
          "shell.execute_reply.started": "2023-04-11T11:23:29.202003Z",
          "shell.execute_reply": "2023-04-11T11:50:13.113148Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 4.681023 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 2\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.54it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 3.138351 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 3\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 2.769298 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 4\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.94it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 1.860718 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 5\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.90it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 1.177217 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 6\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 31.92it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.962108 **\n** Elapsed time: 0:00:17**\n\n----------------------------------------\nEpoch: 7\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.61it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.833593 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 8\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.744632 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 9\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.87it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.672831 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 10\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.618072 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 11\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.33it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.572381 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 12\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.31it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.534339 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 13\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.508776 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 14\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.485931 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 15\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.29it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.464370 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 16\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.448200 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 17\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.431107 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 18\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.420546 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 19\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.409596 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 20\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.87it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.400426 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 21\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.393776 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 22\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.50it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.386476 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 23\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.381232 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 24\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.54it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.374537 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 25\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.74it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.370959 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 26\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.04it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.365333 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 27\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.74it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.361672 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 28\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.67it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.356926 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 29\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.354096 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 30\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.81it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.350345 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 31\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.15it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.346643 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 32\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.48it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.343837 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 33\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.00it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.341441 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 34\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.338780 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 35\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 32.97it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.335788 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 36\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.332672 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 37\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.44it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.330650 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 38\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.40it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.328928 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 39\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.26it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.326640 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 40\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.87it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.325797 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 41\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.322065 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 42\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.321726 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 43\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.57it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.319678 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 44\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.71it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.318457 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 45\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.87it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.315790 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 46\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.68it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.314664 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 47\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.312752 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 48\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.76it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.312396 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 49\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.64it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.310637 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 50\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.309772 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 51\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.46it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.308397 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 52\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.84it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.306878 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 53\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.65it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.305419 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 54\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.71it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.305116 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 55\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.52it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.303821 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 56\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.06it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.301892 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 57\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 31.75it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.301664 **\n** Elapsed time: 0:00:17**\n\n----------------------------------------\nEpoch: 58\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 31.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.300577 **\n** Elapsed time: 0:00:17**\n\n----------------------------------------\nEpoch: 59\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:17, 30.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.299905 **\n** Elapsed time: 0:00:17**\n\n----------------------------------------\nEpoch: 60\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 31.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.297500 **\n** Elapsed time: 0:00:17**\n\n----------------------------------------\nEpoch: 61\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.05it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.297219 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 62\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.91it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.296268 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 63\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.87it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.295804 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 64\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.35it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.294472 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 65\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.62it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.294158 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 66\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.39it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.293315 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 67\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.16it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.292753 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 68\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.72it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.291457 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 69\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.09it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.290483 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 70\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.289621 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 71\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.60it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.290517 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 72\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.31it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.288855 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 73\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.287935 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 74\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.53it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.287867 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 75\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.37it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.286927 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 76\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.286239 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 77\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.10it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.285753 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 78\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.52it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.285099 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 79\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.84it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.285190 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 80\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.284461 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 81\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.283485 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 82\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.08it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.282666 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 83\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.69it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.282487 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 84\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.61it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.281994 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 85\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.281170 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 86\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.78it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.281143 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 87\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.28it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.280882 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 88\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.59it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.280367 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 89\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.280057 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 90\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.57it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.279442 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 91\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.54it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.279053 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 92\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.51it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.278641 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 93\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.278646 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 94\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 32.94it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.277732 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 95\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.25it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.277793 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 96\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.32it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.278123 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 97\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.34it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.277259 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 98\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.56it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.277316 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 99\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:15, 33.82it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.277212 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 100\nRun training...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "527it [00:16, 32.20it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "** End of epoch, accumulated average loss = 0.276403 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1603.30s\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allpreds = []\n",
        "for part, (ids, x, y) in part2ixy.items():\n",
        "    print('\\nClassifying %s set with %d examples ...' % (part, len(x)))\n",
        "    st = time.time()\n",
        "    preds = classify(x, params)\n",
        "    print('%s set classified in %.2fs' % (part, time.time() - st))\n",
        "    count_of_values = list(map(len, preds))\n",
        "    assert np.all(np.array(count_of_values) == top_k)\n",
        "    #score(preds, y)\n",
        "    allpreds.extend(zip(ids, preds))\n",
        "\n",
        "save_preds(allpreds, preds_fname=PREDS_FNAME)\n",
        "print('\\nChecking saved predictions ...')\n",
        "score_preds(preds_path=PREDS_FNAME, data_dir=TRANSLIT_PATH, parts=SCORED_PARTS)"
      ],
      "metadata": {
        "id": "hPELZcXeHLHF",
        "outputId": "51a46330-8402-4fcd-bf0e-437791dbe680",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2023-04-11T11:50:26.682129Z",
          "iopub.execute_input": "2023-04-11T11:50:26.683070Z",
          "iopub.status.idle": "2023-04-11T11:52:09.924299Z",
          "shell.execute_reply.started": "2023-04-11T11:50:26.683016Z",
          "shell.execute_reply": "2023-04-11T11:52:09.923121Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\nClassifying train set with 105371 examples ...\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 527/527 [01:03<00:00,  8.30it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "train set classified in 63.58s\n\nClassifying dev set with 26342 examples ...\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:16<00:00,  8.23it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "dev set classified in 16.05s\n\nClassifying train_small set with 2000 examples ...\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 10/10 [00:01<00:00,  8.45it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "train_small set classified in 1.19s\n\nClassifying dev_small set with 2000 examples ...\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 10/10 [00:01<00:00,  8.21it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "dev_small set classified in 1.22s\n\nClassifying test set with 32926 examples ...\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 165/165 [00:19<00:00,  8.54it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "test set classified in 19.33s\nPredictions saved to preds_translit.tsv\n\nChecking saved predictions ...\ntrain set accuracy@1: 0.67\ndev set accuracy@1: 0.64\ntrain_small set accuracy@1: 0.68\ndev_small set accuracy@1: 0.66\nno labels for test set\n",
          "output_type": "stream"
        },
        {
          "execution_count": 45,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'train': {'acc@1': 0.6660371449450039},\n 'dev': {'acc@1': 0.6421684002733278},\n 'train_small': {'acc@1': 0.675},\n 'dev_small': {'acc@1': 0.6615}}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Hyper-parameters choice"
      ],
      "metadata": {
        "id": "oFfDH0-SsRm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is ready. Now we need to find the optimal hyper-parameters.\n",
        "\n",
        "The quality of models with different hyperparameters should be monitored on dev or on dev_small samples (in order to save time, since generating transliterations is a rather time-consuming process, comparable to one training epoch).\n",
        "\n",
        "To generate predictions, you can use the `generate_predictions` function, to calculate the accuracy@1 metric, and then you can use the `compute_metrics` function.\n",
        "\n",
        "\n",
        "\n",
        "Hyper-parameters are stored in the dictionary `model_config` and `train_config` in train function. The following hyperparameters in `model_config` and `train_config` are suggested to leave unmodified:\n",
        "\n",
        "* n_layers $=$ 2\n",
        "* n_heads $=$ 2\n",
        "* hidden_size $=$ 128\n",
        "* fc_hidden_size $=$ 256\n",
        "* warmup_steps_part $=$ 0.1\n",
        "* batch_size $=$ 200\n",
        "\n",
        " You can vary the dropout value. The model has 4 types of : ***embedding dropout*** applied on embdeddings before sending to the first layer of  Encoder or Decoder, ***attention*** dropout applied on the attention weights in the MultiHeadAttention layer, ***residual dropout*** applied on the output of each sublayer (MultiHeadAttention or FeedForward) in layers Encoder and Decoder and, finaly, ***relu dropout*** in used in FeedForward layer. For all 4 types it is suggested to test the same value of dropout from the list: 0.1, 0.15, 0.2.\n",
        " Also it is suggested to test several peak levels of learning rate - **lr_peak** : 5e-4, 1e-3, 2e-3.\n",
        "\n",
        "Note that if you are using a GPU, then training one epoch takes about 1 minute, and up to 1 GB of video memory is required. When using the CPU, the learning speed slows down by about 2 times. If there are problems with insufficient RAM / video memory, reduce the batch size, but in this case the optimal range of learning rate values will change, and it must be determined again. To train a model with  batch_size $=$ 200 , it will take at least 300 epochs to achieve accuracy 0.66 on dev_small dataset."
      ],
      "metadata": {
        "id": "PxqZbEmtsV0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Question: What are the optimal hyperpameters according to your experiments? Add plots or other descriptions here.* \n",
        "\n",
        "```\n",
        "\n",
        "ENTER HERE YOUR ANSWER\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "IQXVmzk0a60Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER HERE YOUR CODE"
      ],
      "metadata": {
        "id": "ylE7wnrXJqmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters search is performed with label smoothing bellow."
      ],
      "metadata": {
        "id": "sqLlaS_O7vhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label smoothing"
      ],
      "metadata": {
        "id": "7hMYmIO2tf8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We suggest to implement an additional regularization method - **label smoothing**. Now imagine that we have a prediction vector from probabilities at position t in the sequence of tokens for each token id from the vocabulary. CrossEntropy compares it with ground truth one-hot representation\n",
        "\n",
        "$$[0, ... 0, 1, 0, ..., 0].$$\n",
        "\n",
        "And now imagine that we are slightly \"smoothed\" the values in the ground truth vector and obtained\n",
        "\n",
        "$$[\\frac{\\alpha}{|V|}, ..., \\frac{\\alpha}{|V|}, 1(1-\\alpha)+\\frac{\\alpha}{|V|},  \\frac{\\alpha}{|V|}, ... \\frac{\\alpha}{|V|}],$$\n",
        "\n",
        "where $\\alpha$ - parameter from 0 to 1, $|V|$ - vocabulary size - number of components in the ground truth vector. The values ​​of this new vector are still summed to 1. Calculate the cross-entropy of our prediction vector and the new ground truth. Now, firstly, cross-entropy will never reach 0, and secondly, the result of the error function will require the model, as usual, to return the highest probability vector compared to other components of the probability vector for the correct token in the dictionary, but at the same time not too large, because as the value of this probability approaches 1, the value of the error function increases. For research on the use of label smoothing, see the [paper](https://arxiv.org/abs/1906.02629).\n",
        "    \n",
        "Accordingly, in order to embed label smoothing into the model, it is necessary to carry out the transformation described above on the ground truth vectors, as well as to implement the cross-entropy calculation, since the used `torch.nn.CrossEntropy` class is not quite suitable, since for the ground truth representation of `__call__` method takes the id of the correct token and builds a one-hot vector already inside. However, it is possible to implement what is required based on the internal implementation of this class [CrossEntropyLoss](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CrossEntropyLoss).\n",
        "    \n",
        "\n",
        "Test different values of $\\alpha$ (e.x, 0.05, 0.1, 0.2). Describe your experiments and results.\n"
      ],
      "metadata": {
        "id": "setYzbjCtqZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\n",
        "ENTER HERE YOUR ANSWER\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "bL9V_9-7bVzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "\n",
        "top_k = 1\n",
        "part2ixy = load_dataset(TRANSLIT_PATH, parts=SCORED_PARTS)\n",
        "train_ids, train_strings, train_transliterations = part2ixy['train']\n",
        "\n",
        "\n",
        "def objective(trial): \n",
        "    lbl_smooth = trial.suggest_categorical(\"lbl_smooth\", [0.05, 0.1, 0.2])\n",
        "    lr_peak = trial.suggest_categorical(\"lr_peak\", [5e-4, 1e-3, 2e-3])\n",
        "    dropout = trial.suggest_categorical(\"dropout\", [0.1, 0.15, 0.2])\n",
        "        \n",
        "    print('\\nTraining classifier on %d examples from train set ...' % len(train_strings))\n",
        "    st = time.time()\n",
        "    params = train(\n",
        "        train_strings,\n",
        "        train_transliterations,\n",
        "        dropout=dropout,\n",
        "        lr_peak=lr_peak,\n",
        "        label_smoothing=lbl_smooth\n",
        "    )\n",
        "    print('Classifier trained in %.2fs' % (time.time() - st))\n",
        "    \n",
        "    ids, x, y = part2ixy['dev']\n",
        "    preds = classify(x, params)\n",
        "    count_of_values = list(map(len, preds))\n",
        "    assert np.all(np.array(count_of_values) == top_k)\n",
        "    return score(preds, y)['acc@1']\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=30)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-12T09:30:52.394299Z",
          "iopub.execute_input": "2023-04-12T09:30:52.394689Z"
        },
        "trusted": true,
        "id": "EZZtnEde7vhN",
        "outputId": "939cec30-451b-4f7c-b6c7-b7272bce5378"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[32m[I 2023-04-12 09:30:52,989]\u001b[0m A new study created in memory with name: no-name-343c671c-22a3-4328-b869-3a4704e82d5f\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 5.067698 **\n** Elapsed time: 0:00:18**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 2.147160 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 2.082130 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 2.061874 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 2.051508 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1553.19s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.41it/s]\n\u001b[32m[I 2023-04-12 09:57:01,939]\u001b[0m Trial 0 finished with value: 0.613279173942753 and parameters: {'lbl_smooth': 0.2, 'lr_peak': 0.0005, 'dropout': 0.15}. Best is trial 0 with value: 0.613279173942753.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.963721 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 1.035124 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.938736 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.908711 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.893876 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1555.31s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.61it/s]\n\u001b[32m[I 2023-04-12 10:23:12,622]\u001b[0m Trial 1 finished with value: 0.5941462303545668 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.001, 'dropout': 0.2}. Best is trial 0 with value: 0.613279173942753.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.871581 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.914095 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.851248 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.828459 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.817337 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1559.44s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.53it/s]\n\u001b[32m[I 2023-04-12 10:49:27,571]\u001b[0m Trial 2 finished with value: 0.6362842608761673 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.002, 'dropout': 0.1}. Best is trial 2 with value: 0.6362842608761673.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 5.154204 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 2.153248 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 2.085185 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 2.064239 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1560.12s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.72it/s]\n\u001b[32m[I 2023-04-12 11:15:42,873]\u001b[0m Trial 3 finished with value: 0.6068635638903652 and parameters: {'lbl_smooth': 0.2, 'lr_peak': 0.001, 'dropout': 0.15}. Best is trial 2 with value: 0.6362842608761673.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.897480 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.917605 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.852853 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.830261 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.818101 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1558.44s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.69it/s]\n\u001b[32m[I 2023-04-12 11:41:56,538]\u001b[0m Trial 4 finished with value: 0.6390175385316225 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 4 with value: 0.6390175385316225.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.851101 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 1.475935 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 1.380744 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 1.351302 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 1.336615 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1560.32s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.72it/s]\n\u001b[32m[I 2023-04-12 12:08:12,045]\u001b[0m Trial 5 finished with value: 0.585452888922633 and parameters: {'lbl_smooth': 0.1, 'lr_peak': 0.002, 'dropout': 0.2}. Best is trial 4 with value: 0.6390175385316225.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 5.227303 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 2.151421 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 2.084330 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 2.064775 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 2.054377 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1571.27s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.26it/s]\n\u001b[32m[I 2023-04-12 12:34:39,345]\u001b[0m Trial 6 finished with value: 0.6092931440285476 and parameters: {'lbl_smooth': 0.2, 'lr_peak': 0.002, 'dropout': 0.15}. Best is trial 4 with value: 0.6390175385316225.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.801275 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 1.347208 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 1.287240 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 1.266523 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 1.255559 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1567.72s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.45it/s]\n\u001b[32m[I 2023-04-12 13:01:02,733]\u001b[0m Trial 7 finished with value: 0.6374231265659404 and parameters: {'lbl_smooth': 0.1, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 4 with value: 0.6390175385316225.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.979563 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 1.455054 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 1.368760 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 1.341051 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 1.328213 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1584.11s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.37it/s]\n\u001b[32m[I 2023-04-12 13:27:42,674]\u001b[0m Trial 8 finished with value: 0.5893629944575203 and parameters: {'lbl_smooth': 0.1, 'lr_peak': 0.001, 'dropout': 0.2}. Best is trial 4 with value: 0.6390175385316225.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 5.209489 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 2.195262 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 2.124784 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 2.098747 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 2.087331 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1581.76s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.59it/s]\n\u001b[32m[I 2023-04-12 13:54:19,836]\u001b[0m Trial 9 finished with value: 0.5875787715435427 and parameters: {'lbl_smooth': 0.2, 'lr_peak': 0.002, 'dropout': 0.2}. Best is trial 4 with value: 0.6390175385316225.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.642963 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.919188 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.853740 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.830092 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.818524 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1523.34s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.62it/s]\n\u001b[32m[I 2023-04-12 14:19:58,541]\u001b[0m Trial 10 finished with value: 0.6381064459798041 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 4 with value: 0.6390175385316225.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.796873 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.920140 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.854200 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.831346 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.818497 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1512.96s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:14<00:00,  8.84it/s]\n\u001b[32m[I 2023-04-12 14:45:26,490]\u001b[0m Trial 11 finished with value: 0.6389416141523043 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 4 with value: 0.6390175385316225.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.897835 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.915129 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.852679 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.829939 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.818122 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1519.76s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.65it/s]\n\u001b[32m[I 2023-04-12 15:11:01,571]\u001b[0m Trial 12 finished with value: 0.6396628957558272 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 12 with value: 0.6396628957558272.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.473194 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.911997 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.851170 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.829419 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.817061 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1517.79s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.58it/s]\n\u001b[32m[I 2023-04-12 15:36:34,786]\u001b[0m Trial 13 finished with value: 0.6401564042213955 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.783564 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.925560 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.855585 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.831370 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.819724 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1517.71s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.68it/s]\n\u001b[32m[I 2023-04-12 16:02:07,754]\u001b[0m Trial 14 finished with value: 0.6370814668590085 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.690166 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.917599 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.853356 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.830439 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.817602 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1514.67s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.66it/s]\n\u001b[32m[I 2023-04-12 16:27:37,703]\u001b[0m Trial 15 finished with value: 0.6378407106521904 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.692134 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.916666 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.851989 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.830157 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.817644 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1521.09s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.42it/s]\n\u001b[32m[I 2023-04-12 16:53:14,508]\u001b[0m Trial 16 finished with value: 0.6397008579454863 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.566530 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.917555 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.853128 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.830873 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.818240 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1513.64s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.72it/s]\n\u001b[32m[I 2023-04-12 17:18:43,342]\u001b[0m Trial 17 finished with value: 0.6400045554627591 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.733887 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.919421 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.854251 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.830766 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.819195 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1517.05s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.67it/s]\n\u001b[32m[I 2023-04-12 17:44:15,667]\u001b[0m Trial 18 finished with value: 0.6398527067041226 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.812598 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 1.420091 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 1.336383 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 1.311231 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 1.298152 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1515.58s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.55it/s]\n\u001b[32m[I 2023-04-12 18:09:46,728]\u001b[0m Trial 19 finished with value: 0.6075089211145699 and parameters: {'lbl_smooth': 0.1, 'lr_peak': 0.001, 'dropout': 0.15}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.570235 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.915878 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.851890 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.829089 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.818097 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1514.16s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.69it/s]\n\u001b[32m[I 2023-04-12 18:35:16,136]\u001b[0m Trial 20 finished with value: 0.6401564042213955 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.653414 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.920999 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.853958 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.830786 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.818370 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1517.39s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.55it/s]\n\u001b[32m[I 2023-04-12 19:00:49,014]\u001b[0m Trial 21 finished with value: 0.637309239996963 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.583737 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.916325 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.853003 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.830242 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.818735 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1521.69s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.67it/s]\n\u001b[32m[I 2023-04-12 19:26:26,019]\u001b[0m Trial 22 finished with value: 0.6375749753245767 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.863174 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.917168 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.852619 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.829914 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.818400 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1538.57s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.35it/s]\n\u001b[32m[I 2023-04-12 19:52:20,461]\u001b[0m Trial 23 finished with value: 0.6397767823248045 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.704221 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.919100 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.854545 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.831078 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.819540 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1524.75s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.57it/s]\n\u001b[32m[I 2023-04-12 20:18:00,658]\u001b[0m Trial 24 finished with value: 0.6367018449624174 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.757128 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.914513 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.851619 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.829037 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.817716 **\n** Elapsed time: 0:00:15**\nClassifier trained in 1518.52s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.35it/s]\n\u001b[32m[I 2023-04-12 20:43:35,042]\u001b[0m Trial 25 finished with value: 0.6381823703591223 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.661284 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.914308 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.852316 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 61\nRun training...\n** End of epoch, accumulated average loss = 0.830287 **\n** Elapsed time: 0:00:16**\n\n----------------------------------------\nEpoch: 81\nRun training...\n** End of epoch, accumulated average loss = 0.817408 **\n** Elapsed time: 0:00:16**\nClassifier trained in 1524.42s\nUsing GPU device: cuda\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 132/132 [00:15<00:00,  8.73it/s]\n\u001b[32m[I 2023-04-12 21:09:14,639]\u001b[0m Trial 26 finished with value: 0.6393212360488953 and parameters: {'lbl_smooth': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}. Best is trial 13 with value: 0.6401564042213955.\u001b[0m\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraining classifier on 105371 examples from train set ...\nUsing GPU device: cuda\n\n----------------------------------------\nEpoch: 1\nRun training...\n** End of epoch, accumulated average loss = 4.631402 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 21\nRun training...\n** End of epoch, accumulated average loss = 0.917734 **\n** Elapsed time: 0:00:15**\n\n----------------------------------------\nEpoch: 41\nRun training...\n** End of epoch, accumulated average loss = 0.853686 **\n** Elapsed time: 0:00:16**\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the best configuration for more epochs."
      ],
      "metadata": {
        "id": "L6wJwBTK7vhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {'label_smoothing': 0.05, 'lr_peak': 0.0005, 'dropout': 0.1}\n",
        "top_k = 1\n",
        "part2ixy = load_dataset(TRANSLIT_PATH, parts=SCORED_PARTS)\n",
        "train_ids, train_strings, train_transliterations = part2ixy['train']\n",
        "print('\\nTraining classifier on %d examples from train set ...' % len(train_strings))\n",
        "st = time.time()\n",
        "params = train(train_strings, train_transliterations, n_epochs=300, **best_params)\n",
        "print('Classifier trained in %.2fs' % (time.time() - st))"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L2_PL-v7vhO",
        "outputId": "bd10faf0-c3a8-467e-ec77-250d2264b2c4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training classifier on 105371 examples from train set ...\n",
            "Using GPU device: cuda\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 1\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 5.112431 **\n",
            "** Elapsed time: 0:00:19**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 21\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 1.003452 **\n",
            "** Elapsed time: 0:00:15**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 41\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.868775 **\n",
            "** Elapsed time: 0:00:15**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 61\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.835303 **\n",
            "** Elapsed time: 0:00:14**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 81\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.817930 **\n",
            "** Elapsed time: 0:00:15**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 101\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.808008 **\n",
            "** Elapsed time: 0:00:14**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 121\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.799250 **\n",
            "** Elapsed time: 0:00:14**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 141\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.792647 **\n",
            "** Elapsed time: 0:00:15**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 161\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.788202 **\n",
            "** Elapsed time: 0:00:14**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 181\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.783856 **\n",
            "** Elapsed time: 0:00:15**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 201\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.780386 **\n",
            "** Elapsed time: 0:00:14**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 221\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.777339 **\n",
            "** Elapsed time: 0:00:15**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 241\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.774532 **\n",
            "** Elapsed time: 0:00:14**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 261\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.772273 **\n",
            "** Elapsed time: 0:00:14**\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 281\n",
            "Run training...\n",
            "** End of epoch, accumulated average loss = 0.770773 **\n",
            "** Elapsed time: 0:00:15**\n",
            "Classifier trained in 4396.53s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allpreds = []\n",
        "for part, (ids, x, y) in part2ixy.items():\n",
        "    print('\\nClassifying %s set with %d examples ...' % (part, len(x)))\n",
        "    st = time.time()\n",
        "    preds = classify(x, params)\n",
        "    print('%s set classified in %.2fs' % (part, time.time() - st))\n",
        "    count_of_values = list(map(len, preds))\n",
        "    assert np.all(np.array(count_of_values) == top_k)\n",
        "    #score(preds, y)\n",
        "    allpreds.extend(zip(ids, preds))\n",
        "\n",
        "save_preds(allpreds, preds_fname=PREDS_FNAME)\n",
        "print('\\nChecking saved predictions ...')\n",
        "score_preds(preds_path=PREDS_FNAME, data_dir=TRANSLIT_PATH, parts=SCORED_PARTS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2BjaMM-7vhO",
        "outputId": "e09cf683-7d46-4599-c8ff-424b4dea27ea"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classifying train set with 105371 examples ...\n",
            "Using GPU device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 527/527 [01:33<00:00,  5.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set classified in 93.54s\n",
            "\n",
            "Classifying dev set with 26342 examples ...\n",
            "Using GPU device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 132/132 [00:22<00:00,  5.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev set classified in 22.58s\n",
            "\n",
            "Classifying train_small set with 2000 examples ...\n",
            "Using GPU device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:01<00:00,  5.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_small set classified in 1.77s\n",
            "\n",
            "Classifying dev_small set with 2000 examples ...\n",
            "Using GPU device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:02<00:00,  4.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev_small set classified in 2.25s\n",
            "\n",
            "Classifying test set with 32926 examples ...\n",
            "Using GPU device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 165/165 [00:28<00:00,  5.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set classified in 28.50s\n",
            "Predictions saved to preds_translit.tsv\n",
            "\n",
            "Checking saved predictions ...\n",
            "train set accuracy@1: 0.72\n",
            "dev set accuracy@1: 0.67\n",
            "train_small set accuracy@1: 0.72\n",
            "dev_small set accuracy@1: 0.68\n",
            "no labels for test set\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'acc@1': 0.7159370225204278},\n",
              " 'dev': {'acc@1': 0.6661605041378786},\n",
              " 'train_small': {'acc@1': 0.7165},\n",
              " 'dev_small': {'acc@1': 0.6755}}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}